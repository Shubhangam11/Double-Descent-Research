{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DoubleDescent1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1peRGQa1AYPtxAIEE-CXiDm_2wN0jW6GX",
      "authorship_tag": "ABX9TyPr3IcvhsTUq5MunX/vnV3j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhangam11/Double-Descent-Research/blob/master/DoubleDescent1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xfzfSEpYDL8",
        "colab_type": "code",
        "outputId": "65982bb6-b3b2-4cbf-99a8-53365952a3a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/My Drive/noisy_label_understanding_utilizing-master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/noisy_label_understanding_utilizing-master/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSu0pnF3_71g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_e = {}\n",
        "test_err={}\n",
        "train_err={}\n",
        "t_loss={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLJ75Md-pNXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from my_model import create_model\n",
        "from keras.callbacks import Callback, LearningRateScheduler\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import multi_gpu_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "import data\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
        "\n",
        "import argparse\n",
        "#parser = argparse.ArgumentParser()\n",
        "#parser.add_argument('--noise_ratio',type=float)\n",
        "#parser.add_argument('--noise_pattern',type=str)\n",
        "#args = parser.parse_args()\n",
        "\n",
        "\"\"\" parameters \"\"\"\n",
        "noise_ratio = 0#args.noise_ratio\n",
        "noise_pattern = 'sym'#args.noise_pattern #'sym' or 'asym'\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "save_dir = 'Theory'\n",
        "network = 'ResNet20'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir,network+'.h5')\n",
        "print('\\n#######################################\\n noise_ratio: %.2f noise_pattern: %s\\n#######################################\\n'\n",
        "      %(noise_ratio,noise_pattern))\n",
        "\n",
        "#################################################################################################################################\n",
        "\"\"\" Data preparation \"\"\"\n",
        "x_train, y_train, _, _, x_test, y_test = data.prepare_cifar10_data(data_dir='/content/drive/My Drive/noisy_label_understanding_utilizing-master/data/cifar-10-batches-py')#data/cifar-10-batches-py')\n",
        "y_train_noisy = data.flip_label(y_train, pattern=noise_pattern, ratio=noise_ratio, one_hot=True)\n",
        "input_shape = list(x_train.shape[1:])\n",
        "n_classes = y_train.shape[1]\n",
        "n_train = x_train.shape[0]\n",
        "np.save('y_train_total.npy',y_train)\n",
        "np.save('y_train_noisy_total.npy',y_train_noisy)\n",
        "clean_index = np.array([(y_train_noisy[i,:]==y_train[i,:]).all() for i in range(n_train)])# For tracking only, unused during training\n",
        "noisy_index = np.array([not i for i in clean_index])\n",
        "\n",
        "# Generator for data augmantation\n",
        "datagen = ImageDataGenerator(width_shift_range=4./32,  # randomly shift images horizontally (fraction of total width)\n",
        "                             height_shift_range=4./32,  # randomly shift images vertically (fraction of total height)\n",
        "                             horizontal_flip=True\n",
        "                             )  # randomly flip images    \n",
        "\n",
        "\n",
        "\n",
        "#################################################################################################################################\n",
        "\"\"\" Build model \"\"\"\n",
        "\n",
        "val_idx = np.array([True for i in range(n_train)])\n",
        "val_idx_int = np.array([i for i in range(n_train) if val_idx[i]]) # integer index\n",
        "np.random.shuffle(val_idx_int)\n",
        "n_val_tenth = int(np.sum(val_idx)/10)\n",
        "val1_idx = val_idx_int[:n_val_tenth] # integer index\n",
        "val2_idx = val_idx_int[n_val_tenth:] # integer index\n",
        "\n",
        "#checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_acc', verbose=1, save_best_only=False)\n",
        "\n",
        "class Noisy_acc(Callback):\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        idx = val2_idx[np.random.choice(len(val2_idx),1000)] # train on the first half while test on the second half \n",
        "        \n",
        "        predict = self.model.predict(x_train[idx,:])\n",
        "        predict = np.argmax(predict,axis=1)\n",
        "        _acc_mix = accuracy_score(np.argmax(y_train_noisy[idx,:],axis=1), predict)\n",
        "        _acc_clean = accuracy_score(np.argmax(y_train_noisy[idx,:][clean_index[idx],:],axis=1), predict[clean_index[idx]])\n",
        "        _acc_noisy = accuracy_score(np.argmax(y_train_noisy[idx,:][noisy_index[idx],:],axis=1), predict[noisy_index[idx]])\n",
        "\n",
        "        print(\"- acc_mix: %.4f - acc_clean: %.4f - acc_noisy: %.4f\\n\" % (_acc_mix, _acc_clean, _acc_noisy))\n",
        "        return\n",
        "noisy_acc = Noisy_acc()\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    # Learning Rate Schedule\n",
        "    lr = 1e-3\n",
        "    #if epoch > 180:\n",
        "     #   lr *= 0.5e-3\n",
        "    #elif epoch > 160:\n",
        "     #   lr *= 1e-3\n",
        "   # elif epoch > 120:\n",
        "    #    lr *= 1e-2\n",
        "    #elif epoch > 100:\n",
        "     #   lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "lr_callback = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "\n",
        "# Define optimizer and compile model\n",
        "optimizer = optimizers.Adam(lr_schedule(0))#, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "\n",
        "for s in range(1,64):\n",
        "  model = create_model(input_shape=input_shape, classes=n_classes, name=network,k=s, architecture=network)\n",
        "  model.summary()\n",
        "\n",
        "#parallel_model = multi_gpu_model(model, gpus=2)\n",
        "  model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "##################################################################################################################################\n",
        "\n",
        "  results = model.fit_generator(datagen.flow(x_train[val1_idx,:], y_train_noisy[val1_idx,:], batch_size = batch_size),\n",
        "                               epochs = epochs,\n",
        "                               validation_data=(x_train[val2_idx,:], y_train_noisy[val2_idx,:]),\n",
        "                               callbacks=[noisy_acc, lr_callback])\n",
        "\n",
        "\n",
        "  rr=[]\n",
        "  for t in results.history['val_accuracy']:\n",
        "    rr.append(1-t)\n",
        "  test_err7[s]=rr\n",
        "  rs=[] \n",
        "  for t in results.history['accuracy']:\n",
        "    rs.append(1-t) \n",
        "    train_err[s]=rs\n",
        "  \n",
        "  rl=[] \n",
        "  for t in results.history['loss']: \n",
        "  rl.append(1-t) \n",
        "  t_loss[s]=rl\n",
        "\n",
        "  scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test loss:', scores[0])\n",
        "  print('Test accuracy:', scores[1])\n",
        "  test_e[s] = 1 - scores[1]\n",
        "\n",
        "\n",
        "  y_pred = np.argmax(model.predict(x_train[val2_idx,:]), axis=1)\n",
        "  y_true_noisy = np.argmax(y_train_noisy[val2_idx,:],axis=1)\n",
        "  select_idx =  val2_idx[y_pred==y_true_noisy]# integer index\n",
        "  print('Noisy Validation Accuracy: %.4f'%(len(select_idx)/len(y_pred)))\n",
        "  print('Label Precision: %.4f'%(np.sum(clean_index[select_idx])/len(select_idx)))\n",
        "  print('Label Recall: %.4f'%(np.sum(clean_index[select_idx])/np.sum(clean_index[val2_idx])))\n",
        "\n",
        "  y_test_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "  np.save(save_dir+'/y_pred_'+noise_pattern+str(noise_ratio)+'.npy',y_pred)\n",
        "  np.save(save_dir+'/y_true_'+noise_pattern+str(noise_ratio)+'.npy',np.argmax(y_train[val2_idx,:], axis=1))\n",
        "  np.save(save_dir+'/y_test_pred_'+noise_pattern+str(noise_ratio)+'.npy',y_test_pred)\n",
        "  np.save(save_dir+'/y_test_true_'+noise_pattern+str(noise_ratio)+'.npy',np.argmax(y_test, axis=1))\n",
        "  print('Noise ratio: %.2f'%noise_ratio)\n",
        "\n",
        "  model.save(filepath)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB4nYUX7RWV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ten_test={}\n",
        "for k,v in enumerate(arr):\n",
        "  ten_test[v] = test_e[v]\n",
        "print(ten_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHxXc4VO-lxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_err7={4: [0.7908799946308136, 0.5669600069522858, 0.5259599983692169, 0.5854400098323822, 0.5220000147819519, 0.5359599888324738, 0.48552000522613525, 0.4954800009727478, 0.4588800072669983, 0.4495999813079834, 0.43115997314453125, 0.4477599859237671, 0.42659997940063477, 0.4115599989891052, 0.41196000576019287, 0.38679999113082886, 0.40935999155044556, 0.43084001541137695, 0.39535999298095703, 0.41975998878479004, 0.37536001205444336, 0.3747599720954895, 0.4376800060272217, 0.3723599910736084, 0.3609200119972229, 0.39955997467041016, 0.39604002237319946, 0.36448001861572266, 0.384880006313324, 0.35199999809265137, 0.3433600068092346, 0.4031199812889099, 0.32631999254226685, 0.3537600040435791, 0.36984002590179443, 0.3266400098800659, 0.36607998609542847, 0.31084001064300537, 0.3285199999809265, 0.32204002141952515, 0.30699998140335083, 0.31032001972198486, 0.38235998153686523, 0.3317199945449829, 0.33903998136520386, 0.30831998586654663, 0.34935998916625977, 0.3336399793624878, 0.31624001264572144, 0.3001199960708618], 6: [0.7358399927616119, 0.5140399932861328, 0.4567199945449829, 0.4509199857711792, 0.43639999628067017, 0.40755999088287354, 0.4036800265312195, 0.4214400053024292, 0.41383999586105347, 0.3633599877357483, 0.4174799919128418, 0.3720800280570984, 0.3511599898338318, 0.4166799783706665, 0.36163997650146484, 0.3182799816131592, 0.32264000177383423, 0.3243600130081177, 0.35071998834609985, 0.3484399914741516, 0.326960027217865, 0.30743998289108276, 0.3182799816131592, 0.2988799810409546, 0.3423200249671936, 0.29875999689102173, 0.2670000195503235, 0.2906799912452698, 0.2969200015068054, 0.29027998447418213, 0.28600001335144043, 0.2685999870300293, 0.2710000276565552, 0.285319983959198, 0.3278399705886841, 0.2723199725151062, 0.26183998584747314, 0.28703999519348145, 0.28008002042770386, 0.28060001134872437, 0.28487998247146606, 0.2881600260734558, 0.26784002780914307, 0.2803199887275696, 0.2749999761581421, 0.28408002853393555, 0.2688000202178955, 0.27139997482299805, 0.24032002687454224, 0.24676001071929932], 8: [0.6608400046825409, 0.5938000082969666, 0.4633200168609619, 0.4208800196647644, 0.4138000011444092, 0.35391998291015625, 0.39028000831604004, 0.3733999729156494, 0.3690800070762634, 0.320360004901886, 0.3452399969100952, 0.35979998111724854, 0.3726000189781189, 0.3245599865913391, 0.3047599792480469, 0.3736400008201599, 0.3621600270271301, 0.32239997386932373, 0.27060002088546753, 0.296239972114563, 0.3306400179862976, 0.2564399838447571, 0.27556002140045166, 0.31327998638153076, 0.25787997245788574, 0.28115999698638916, 0.2666800022125244, 0.25540000200271606, 0.2672399878501892, 0.2616400122642517, 0.3065599799156189, 0.28196001052856445, 0.2649199962615967, 0.23956000804901123, 0.23711997270584106, 0.24115997552871704, 0.2404400110244751, 0.2781199812889099, 0.2596399784088135, 0.24331998825073242, 0.26260000467300415, 0.22943997383117676, 0.21872001886367798, 0.2574399709701538, 0.2351199984550476, 0.2635200023651123, 0.23983997106552124, 0.2247599959373474, 0.2175999879837036, 0.24352002143859863], 10: [0.8161599934101105, 0.5607199966907501, 0.4742000102996826, 0.47152000665664673, 0.4168800115585327, 0.49563997983932495, 0.4342799782752991, 0.3630800247192383, 0.39740002155303955, 0.34780001640319824, 0.3306800127029419, 0.28196001052856445, 0.2773200273513794, 0.3156399726867676, 0.2991200089454651, 0.281279981136322, 0.2856000065803528, 0.2610800266265869, 0.2909200191497803, 0.24959999322891235, 0.2534000277519226, 0.25224000215530396, 0.2696400284767151, 0.25019997358322144, 0.29044002294540405, 0.28839999437332153, 0.2874799966812134, 0.26340001821517944, 0.2449600100517273, 0.2449600100517273, 0.24844002723693848, 0.23483997583389282, 0.24383997917175293, 0.259880006313324, 0.23820000886917114, 0.2319599986076355, 0.23356002569198608, 0.21460002660751343, 0.2537199854850769, 0.21571999788284302, 0.2171199917793274, 0.2147200107574463, 0.20216000080108643, 0.2104799747467041, 0.23820000886917114, 0.22420001029968262, 0.25252002477645874, 0.2396799921989441, 0.22100001573562622, 0.22751998901367188], 12: [0.6449199914932251, 0.5203999876976013, 0.4711199998855591, 0.4262400269508362, 0.5054799914360046, 0.3617600202560425, 0.36768001317977905, 0.3338800072669983, 0.3862000107765198, 0.33796000480651855, 0.32896000146865845, 0.26495999097824097, 0.28491997718811035, 0.25731998682022095, 0.2775599956512451, 0.3338800072669983, 0.31331998109817505, 0.25756001472473145, 0.30375999212265015, 0.3229600191116333, 0.2884399890899658, 0.25120002031326294, 0.2549999952316284, 0.2508400082588196, 0.24427998065948486, 0.24540001153945923, 0.24607998132705688, 0.26947999000549316, 0.22196000814437866, 0.25352001190185547, 0.285319983959198, 0.20547997951507568, 0.2658799886703491, 0.22575998306274414, 0.23452001810073853, 0.25195997953414917, 0.22696000337600708, 0.22092002630233765, 0.2807999849319458, 0.2595999836921692, 0.22395998239517212, 0.2569599747657776, 0.22496002912521362, 0.18699997663497925, 0.21568000316619873, 0.21612000465393066, 0.21119999885559082, 0.24195998907089233, 0.26972001791000366, 0.1998400092124939], 14: [0.7672799974679947, 0.5110799968242645, 0.5117200016975403, 0.3906800150871277, 0.40671998262405396, 0.35471999645233154, 0.32572001218795776, 0.33267998695373535, 0.30379998683929443, 0.3145599961280823, 0.27583998441696167, 0.3280799984931946, 0.3452000021934509, 0.2847200036048889, 0.2855600118637085, 0.26867997646331787, 0.3208400011062622, 0.25015997886657715, 0.255079984664917, 0.2502400279045105, 0.24251997470855713, 0.24908000230789185, 0.23448002338409424, 0.24927997589111328, 0.21095997095108032, 0.22355997562408447, 0.249239981174469, 0.27056002616882324, 0.24620002508163452, 0.2178400158882141, 0.2558799982070923, 0.22539997100830078, 0.2367200255393982, 0.20051997900009155, 0.23111999034881592, 0.23159998655319214, 0.22676002979278564, 0.2308800220489502, 0.2204800248146057, 0.22075998783111572, 0.20608001947402954, 0.21376001834869385, 0.19700002670288086, 0.22619998455047607, 0.24323999881744385, 0.20551997423171997, 0.20759999752044678, 0.21411997079849243, 0.20388001203536987, 0.1982799768447876], 16: [0.6474800109863281, 0.5712000131607056, 0.42239999771118164, 0.46104001998901367, 0.3989199995994568, 0.40219998359680176, 0.34592002630233765, 0.3073599934577942, 0.3051599860191345, 0.30423998832702637, 0.4013199806213379, 0.313759982585907, 0.30636000633239746, 0.2993199825286865, 0.2717599868774414, 0.2927600145339966, 0.2726399898529053, 0.24904000759124756, 0.24456000328063965, 0.2669600248336792, 0.24067997932434082, 0.21872001886367798, 0.22416001558303833, 0.2234399914741516, 0.2597600221633911, 0.24396002292633057, 0.28255999088287354, 0.25492000579833984, 0.2274399995803833, 0.21351999044418335, 0.2566400170326233, 0.23475998640060425, 0.2229599952697754, 0.23852002620697021, 0.23015999794006348, 0.23707997798919678, 0.23659998178482056, 0.20323997735977173, 0.2359200119972229, 0.20451998710632324, 0.20811998844146729, 0.22255998849868774, 0.22223997116088867, 0.23563998937606812, 0.23416000604629517, 0.19312000274658203, 0.19340002536773682, 0.20524001121520996, 0.18911999464035034, 0.1969199776649475], 18: [0.7945200055837631, 0.6317600011825562, 0.576119989156723, 0.4307199716567993, 0.4771599769592285, 0.3744000196456909, 0.3694000244140625, 0.30875998735427856, 0.3169599771499634, 0.29232001304626465, 0.2897599935531616, 0.298520028591156, 0.30004000663757324, 0.28968000411987305, 0.314520001411438, 0.2571600079536438, 0.24712002277374268, 0.285040020942688, 0.2152000069618225, 0.23360002040863037, 0.2195199728012085, 0.2491999864578247, 0.22759997844696045, 0.2436000108718872, 0.24136000871658325, 0.24739998579025269, 0.2649199962615967, 0.20951998233795166, 0.20943999290466309, 0.24220001697540283, 0.21583998203277588, 0.23368000984191895, 0.22307997941970825, 0.26951998472213745, 0.2160000205039978, 0.2147200107574463, 0.24076002836227417, 0.19467997550964355, 0.20028001070022583, 0.19811999797821045, 0.18691998720169067, 0.22148001194000244, 0.2149999737739563, 0.2043200135231018, 0.20667999982833862, 0.19315999746322632, 0.19572001695632935, 0.18624001741409302, 0.2067199945449829, 0.19863998889923096], 20: [0.7240400016307831, 0.4934399724006653, 0.4397600293159485, 0.495959997177124, 0.4145200252532959, 0.3911600112915039, 0.37599998712539673, 0.30751997232437134, 0.3138800263404846, 0.35971999168395996, 0.2672799825668335, 0.24067997932434082, 0.2892799973487854, 0.2585200071334839, 0.26392000913619995, 0.2906399965286255, 0.24952000379562378, 0.23827999830245972, 0.2136399745941162, 0.2677599787712097, 0.2409999966621399, 0.22759997844696045, 0.2422800064086914, 0.2358800172805786, 0.31164002418518066, 0.2398800253868103, 0.2529600262641907, 0.2215999960899353, 0.23492002487182617, 0.2072799801826477, 0.23676002025604248, 0.21135997772216797, 0.2593200206756592, 0.2120000123977661, 0.20520001649856567, 0.22676002979278564, 0.20579999685287476, 0.22144001722335815, 0.18655997514724731, 0.21751999855041504, 0.23056000471115112, 0.24383997917175293, 0.20200002193450928, 0.21612000465393066, 0.20175999402999878, 0.18228000402450562, 0.2027999758720398, 0.18636000156402588, 0.19415998458862305, 0.1889200210571289], 24: [0.7009600102901459, 0.5009999871253967, 0.4524000287055969, 0.398360013961792, 0.46008002758026123, 0.4034000039100647, 0.38040000200271606, 0.3672400116920471, 0.3227999806404114, 0.3476399779319763, 0.2433599829673767, 0.3158400058746338, 0.23351997137069702, 0.2713199853897095, 0.2597600221633911, 0.2409999966621399, 0.2836800217628479, 0.24503999948501587, 0.2563999891281128, 0.24940001964569092, 0.2698400020599365, 0.26187998056411743, 0.268559992313385, 0.2114800214767456, 0.2120400071144104, 0.2656800150871277, 0.209119975566864, 0.25436002016067505, 0.227400004863739, 0.22539997100830078, 0.20363998413085938, 0.19779998064041138, 0.20099997520446777, 0.20579999685287476, 0.20007997751235962, 0.19555997848510742, 0.18115997314453125, 0.22835999727249146, 0.18567997217178345, 0.2576799988746643, 0.1993200182914734, 0.2003999948501587, 0.2436400055885315, 0.17996001243591309, 0.17536002397537231, 0.21483999490737915, 0.19815999269485474, 0.17935997247695923, 0.2218000292778015, 0.2056400179862976], 28: [0.7897599935531616, 0.4820399880409241, 0.43136000633239746, 0.3821200132369995, 0.3169599771499634, 0.38708001375198364, 0.3960000276565552, 0.3415200114250183, 0.29875999689102173, 0.29707998037338257, 0.2707200050354004, 0.2731199860572815, 0.3635600209236145, 0.2619199752807617, 0.2943999767303467, 0.23615998029708862, 0.22776001691818237, 0.30435997247695923, 0.22703999280929565, 0.23015999794006348, 0.2067599892616272, 0.18984001874923706, 0.2783200144767761, 0.18844002485275269, 0.20572000741958618, 0.24067997932434082, 0.25672000646591187, 0.2176799774169922, 0.19599997997283936, 0.2210400104522705, 0.1980000138282776, 0.2229200005531311, 0.23640000820159912, 0.3065999746322632, 0.18936002254486084, 0.19911998510360718, 0.21700000762939453, 0.20200002193450928, 0.19260001182556152, 0.21384000778198242, 0.23356002569198608, 0.2160400152206421, 0.17132002115249634, 0.2144799828529358, 0.18747997283935547, 0.18860000371932983, 0.166159987449646, 0.20271998643875122, 0.2112399935722351, 0.21851998567581177], 32: [0.7404800057411194, 0.5445199906826019, 0.4413999915122986, 0.34747999906539917, 0.3519600033760071, 0.43904000520706177, 0.3126800060272217, 0.3264399766921997, 0.3346400260925293, 0.31643998622894287, 0.28439998626708984, 0.2780799865722656, 0.2519199848175049, 0.22527998685836792, 0.2724800109863281, 0.2520400285720825, 0.21227997541427612, 0.20844000577926636, 0.2282400131225586, 0.281279981136322, 0.23471999168395996, 0.2918400168418884, 0.21188002824783325, 0.28144001960754395, 0.20876002311706543, 0.22496002912521362, 0.22391998767852783, 0.20131999254226685, 0.20327997207641602, 0.20855998992919922, 0.17764002084732056, 0.24067997932434082, 0.2216399908065796, 0.21824002265930176, 0.21740001440048218, 0.23176002502441406, 0.19283998012542725, 0.22464001178741455, 0.20579999685287476, 0.2112399935722351, 0.29711997509002686, 0.18004000186920166, 0.17575997114181519, 0.1932399868965149, 0.20976001024246216, 0.166920006275177, 0.18655997514724731, 0.18883997201919556, 0.25540000200271606, 0.1934800148010254], 48: [0.6972000002861023, 0.6022000014781952, 0.44700002670288086, 0.40855997800827026, 0.3908799886703491, 0.45976001024246216, 0.33428001403808594, 0.32264000177383423, 0.3216000199317932, 0.3893200159072876, 0.26848000288009644, 0.35975998640060425, 0.29732000827789307, 0.3386800289154053, 0.28724002838134766, 0.2874799966812134, 0.29395997524261475, 0.2786800265312195, 0.2797999978065491, 0.27139997482299805, 0.3903599977493286, 0.25727999210357666, 0.2990800142288208, 0.24479997158050537, 0.2946400046348572, 0.2242400050163269, 0.23215997219085693, 0.2820799946784973, 0.24427998065948486, 0.24664002656936646, 0.18747997283935547, 0.19095999002456665, 0.26332002878189087, 0.19896000623703003, 0.2295600175857544, 0.24795997142791748, 0.1934800148010254, 0.23896002769470215, 0.20719999074935913, 0.24563997983932495, 0.17603999376296997, 0.1815599799156189, 0.2043200135231018, 0.23052000999450684, 0.2242799997329712, 0.19628000259399414, 0.17423999309539795, 0.18772000074386597, 0.16531997919082642, 0.21556001901626587], 64: [0.7512000054121017, 0.5503199994564056, 0.48236000537872314, 0.5165599882602692, 0.42611998319625854, 0.4213600158691406, 0.39823997020721436, 0.4089599847793579, 0.3872399926185608, 0.39420002698898315, 0.36799997091293335, 0.34671998023986816, 0.3008800148963928, 0.2750399708747864, 0.26152002811431885, 0.27428001165390015, 0.26371997594833374, 0.2796400189399719, 0.31704002618789673, 0.29232001304626465, 0.2215999960899353, 0.21964001655578613, 0.21024000644683838, 0.201479971408844, 0.23236000537872314, 0.21684002876281738, 0.27104002237319946, 0.2173200249671936, 0.20951998233795166, 0.21460002660751343, 0.20055997371673584, 0.2796800136566162, 0.2279999852180481, 0.24695998430252075, 0.2534400224685669, 0.2441999912261963, 0.23027998208999634, 0.18111997842788696, 0.19871997833251953, 0.2553200125694275, 0.2353600263595581, 0.2093999981880188, 0.195360004901886, 0.17735999822616577, 0.16923999786376953, 0.2091599702835083, 0.22387999296188354, 0.18676000833511353, 0.19863998889923096, 0.23308002948760986]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZRMLSQI4mCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for t in results.history['val_accuracy']:\n",
        "  test_err5.append(1-t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhgvUyCv4cq4",
        "colab_type": "code",
        "outputId": "3d226cae-fc1d-484e-ae4a-4b9cd0662b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "tr =sorted(ten_test.items())\n",
        "x10,y10 = zip(*tr)\n",
        "plt.plot(x10,y10,label='Resnetv2-20')\n",
        "  \n",
        "plt.xlabel('Width Parameter')\n",
        "plt.ylabel('Test Error')\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3c97f3dc88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qqxX3f8X4w-k",
        "colab": {}
      },
      "source": [
        "\n",
        "import keras\n",
        "from keras.layers import Input,Conv2D,Dense,BatchNormalization,Activation,add,GlobalAveragePooling2D,GlobalMaxPooling2D\n",
        "from keras.layers import AveragePooling2D, Flatten\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "# ResNet building block of two layers\n",
        "def building_block(X, filter_size, filters, stride=1):\n",
        "\n",
        "    # Save the input value for shortcut\n",
        "    X_shortcut = X\n",
        "\n",
        "    # Reshape shortcut for later adding if dimensions change\n",
        "    #if stride > 1:\n",
        "    if stride > 1 or filters>X.get_shape()[-1]:\n",
        "\n",
        "        X_shortcut = Conv2D(filters, (1, 1), strides=stride, padding='same', kernel_regularizer=l2(1e-4))(X_shortcut)\n",
        "        X_shortcut = BatchNormalization(axis=3)(X_shortcut)\n",
        "\n",
        "    # First layer of the block\n",
        "    X = Conv2D(filters, kernel_size = filter_size, strides=stride, padding='same', kernel_regularizer=l2(1e-4))(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second layer of the block\n",
        "    X = Conv2D(filters, kernel_size = filter_size, strides=(1, 1), padding='same', kernel_regularizer=l2(1e-4))(X)\n",
        "    X = BatchNormalization(axis=3)(X)\n",
        "    X = add([X, X_shortcut])  # Add shortcut value to main path\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "# Full model\n",
        "def create_model(input_shape, classes, name,k, architecture='ResNet32'):\n",
        "\n",
        "    # Define the input\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    if architecture == 'ResNet32':\n",
        "        # Stage 1\n",
        "        X = Conv2D(filters=16, kernel_size=3, strides=(1, 1), padding='same')(X_input)\n",
        "        X = BatchNormalization(axis=3)(X)\n",
        "        X = Activation('relu')(X)\n",
        "    \n",
        "        # Stage 2\n",
        "        X = building_block(X, filter_size=3, filters=16, stride=1)\n",
        "        X = building_block(X, filter_size=3, filters=16, stride=1)\n",
        "        X = building_block(X, filter_size=3, filters=16, stride=1)\n",
        "        X = building_block(X, filter_size=3, filters=16, stride=1)\n",
        "        X = building_block(X, filter_size=3, filters=16, stride=1)\n",
        "    \n",
        "        # Stage 3\n",
        "        X = building_block(X, filter_size=3, filters=32, stride=2)  # dimensions change (stride=2)\n",
        "        X = building_block(X, filter_size=3, filters=32, stride=1)\n",
        "        X = building_block(X, filter_size=3, filters=32, stride=1)\n",
        "        X = building_block(X, filter_size=3, filters=32, stride=1)\n",
        "        X = building_block(X, filter_size=3, filters=32, stride=1)\n",
        "    \n",
        "        # Stage 4\n",
        "        X = building_block(X, filter_size=3, filters=64, stride=2)  # dimensions change (stride=2)\n",
        "        X = building_block(X, filter_size=3, filters=64, stride=1)\n",
        "        X = building_block(X, filter_size=3, filters=64, stride=1)\n",
        "        X = building_block(X, filter_size=3, filters=64, stride=1)\n",
        "        X = building_block(X, filter_size=3, filters=64, stride=1)\n",
        "    \n",
        "        # Average pooling and output layer\n",
        "        X = GlobalAveragePooling2D()(X) \n",
        "        X = Dense(classes, activation='softmax')(X)\n",
        "        \n",
        "    elif architecture == 'WRN-28-10':\n",
        "        \n",
        "        # Stage 1\n",
        "        X = Conv2D(filters=16, kernel_size=3, strides=(1, 1), padding='same')(X_input)\n",
        "        X = BatchNormalization(axis=3)(X)\n",
        "        X = Activation('relu')(X)\n",
        "\n",
        "        # Stage 2\n",
        "        for i in range(9):\n",
        "            X = building_block(X, filter_size=3, filters=160, stride=1)\n",
        "\n",
        "        # Stage 3\n",
        "        X = building_block(X, filter_size=3, filters=320, stride=2)  # dimensions change (stride=2)\n",
        "        for i in range(1,9):\n",
        "            X = building_block(X, filter_size=3, filters=320, stride=1)\n",
        "\n",
        "        # Stage 4\n",
        "        X = building_block(X, filter_size=3, filters=640, stride=2)  # dimensions change (stride=2)\n",
        "        for i in range(1,9):\n",
        "            X = building_block(X, filter_size=3, filters=640, stride=1)        \n",
        "        \n",
        "        # Average pooling and output layer\n",
        "        X = GlobalAveragePooling2D()(X)\n",
        "        X = Dense(classes, activation='softmax')(X)\n",
        "    \n",
        "    elif architecture == 'ResNet110':\n",
        "        return resnet_v2(input_shape, depth=110, num_classes=classes)\n",
        "    elif architecture == 'ResNet20':\n",
        "      return resnet_v2(input_shape,k, depth=20, num_classes=classes)\n",
        "    \n",
        "    elif architecture == 'ResNet164':\n",
        "        return resnet_v2(input_shape, depth=164, num_classes=classes)\n",
        "        \n",
        "    elif architecture == 'CNN9':        \n",
        "        X = Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='same', kernel_regularizer=l2(1e-4))(X_input)\n",
        "        X = BatchNormalization(axis=3)(X)\n",
        "        X = Activation('relu')(X)\n",
        "        X = Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='same', kernel_regularizer=l2(1e-4))(X)\n",
        "        X = BatchNormalization(axis=3)(X)\n",
        "        X = Activation('relu')(X)\n",
        "        X = Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='same', kernel_regularizer=l2(1e-4))(X)\n",
        "        X = BatchNormalization(axis=3)(X)\n",
        "        X = Activation('relu')(X)\n",
        "        X = GlobalMaxPooling2D()(X)\n",
        "        \n",
        "        X = Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='same', kernel_regularizer=l2(1e-4))(X)\n",
        "        X = BatchNormalization(axis=3)(X)\n",
        "        X = Activation('relu')(X)\n",
        "        X = Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='same', kernel_regularizer=l2(1e-4))(X)\n",
        "        X = BatchNormalization(axis=3)(X)\n",
        "        X = Activation('relu')(X)\n",
        "        X = Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='same', kernel_regularizer=l2(1e-4))(X)\n",
        "        X = BatchNormalization(axis=3)(X)\n",
        "        X = Activation('relu')(X)\n",
        "        X = GlobalMaxPooling2D()(X)\n",
        "        \n",
        "        X = Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same', kernel_regularizer=l2(1e-4))(X)\n",
        "        X = BatchNormalization(axis=3)(X)\n",
        "        X = Activation('relu')(X)\n",
        "        X = Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='same', kernel_regularizer=l2(1e-4))(X)\n",
        "        X = BatchNormalization(axis=3)(X)\n",
        "        X = Activation('relu')(X)\n",
        "        X = Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='same', kernel_regularizer=l2(1e-4))(X)\n",
        "        X = BatchNormalization(axis=3)(X)\n",
        "        X = Activation('relu')(X)\n",
        "\n",
        "        # Average pooling and output layer\n",
        "        X = GlobalAveragePooling2D()(X)\n",
        "        X = Dense(classes, activation='softmax')(X)\n",
        "        \n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=X_input, outputs=X, name=name)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_G8KRLv24dgI",
        "colab": {}
      },
      "source": [
        "\n",
        "# borrow from https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  #kernel_regularizer=l2(1e-4)\n",
        "                  )\n",
        "\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def resnet_v2(input_shape,k, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = k\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrdW1tA14UwU",
        "colab_type": "text"
      },
      "source": [
        "**Retired Work**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6nLp6MrM_qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#th_dict=[]\n",
        "tes=[]\n",
        "ite =2\n",
        "for ind,val in enumerate(results.history['val_accuracy']):\n",
        "  th_dict[ite] = val\n",
        "  te.append(1-val)\n",
        "  if ind== 100:\n",
        "    ite+=1\n",
        "plt.plot(th,label='Resnetv2-20 Width Parameter-64')\n",
        "plt.xlim([0,100])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Error')\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLYsYR48grK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.signal\n",
        "import matplotlib.pyplot as plt\n",
        "x=[]\n",
        "x1=[]\n",
        "y1=[]\n",
        "y=[]\n",
        "x2=[]\n",
        "y2=[]\n",
        "#for i,v in enumerate(test_err6):\n",
        " # x2.append(i+1)\n",
        "  #y2.append(v)\n",
        "#for i,v in enumerate(test_err5):\n",
        " # x.append(i+1)\n",
        "  #y.append(v)\n",
        "for i,v in test_e.items():\n",
        "  x1.append(i)\n",
        "  y1.append(v)\n",
        "#yhat = scipy.signal.savgol_filter(y, 21, 1)\n",
        "yhat1 = scipy.signal.savgol_filter(yq, 9, 2)\n",
        "#yhat2 = scipy.signal.savgol_filter(y2, 11, 1)\n",
        "#th_dict=[]\n",
        "#te=[]\n",
        "#ite =1\n",
        "#for ind,val in enumerate(results.history['val_accuracy']):\n",
        " # th_dict[ite] = val\n",
        "  #te.append(1-val)\n",
        "  #if ind== 100:\n",
        "   # ite+=1\n",
        "plt.plot(xq,yhat1,label='Resnetv2-20  Noise Ratio:0')\n",
        "#plt.plot(x,yhat1,label='Resnetv2-20 Parameter Width= 16')\n",
        "#plt.plot(x,yhat,label='Resnetv2-20 Parameter Width= 64')\n",
        "#plt.annotate('Interpolation Threshold', xy=(75,0.474), xytext=(75, 0.5),\n",
        " #            arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "  #           )\n",
        "plt.xlim([0,64])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Error')\n",
        "plt.legend(loc='upper right')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgVpA7HpvrzE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT_BGFp0vJtj",
        "colab_type": "code",
        "outputId": "b93e2468-9e44-41d2-a0de-b4b514c48a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "import numpy as np\n",
        "import scipy.signal\n",
        "import matplotlib.pyplot as plt\n",
        "x=[]\n",
        "x1=[]\n",
        "y1=[]\n",
        "y=[]\n",
        "x2=[]\n",
        "y2=[]\n",
        "#for i,v in enumerate(test_err6):\n",
        " # x2.append(i+1)\n",
        "  #y2.append(v)\n",
        "#for i,v in enumerate(test_err5):\n",
        " # x.append(i+1)\n",
        "  #y.append(v)\n",
        "for i,v in tent.items():\n",
        "  x1.append(i)\n",
        "  y1.append(v)\n",
        "#yhat = scipy.signal.savgol_filter(y, 21, 1)\n",
        "yhat1 = scipy.signal.savgol_filter(yt, 5, 2)\n",
        "#yhat2 = scipy.signal.savgol_filter(yq, 5, 3)\n",
        "#th_dict=[]\n",
        "#te=[]\n",
        "fig, ax = plt.subplots()\n",
        "#ite =1\n",
        "#for ind,val in enumerate(results.history['val_accuracy']):\n",
        " # th_dict[ite] = val\n",
        "  #te.append(1-val)\n",
        "  #if ind== 100:\n",
        "   # ite+=1\n",
        "#plt.plot(ks, errs, label='Test Error')\n",
        "#ax.axvspan(21, 28, alpha=0.5, color='Red')\n",
        "plt.plot(xt,yhat1,label='Resnetv2-20 5000 Samples')\n",
        "#plt.plot(xq,yhat2,label='Resnetv2-20 25000 Samples')\n",
        "\n",
        "#plt.plot(x,yhat,label='Resnetv2-20 Parameter Width= 64')\n",
        "#plt.annotate('Interpolation Threshold', xy=(28,0.3), xytext=(28, 0.35),\n",
        " #            arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "  #           )\n",
        "plt.xlim([0,64])\n",
        "plt.xlabel('Model Width Parameter')\n",
        "plt.ylabel('Test Error')\n",
        "plt.legend(loc='upper right')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f56f0ef6588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkJA0bYYvJ7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoMoqdDnRvDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum_dict[1]=yy\n",
        "sum_dict[2]=test_err1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT9IlZjzSe4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sd=[]\n",
        "for i in range(0,99):\n",
        "  sd.append((yy[i]+test_err1[i])/10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GxphVdzUkIW",
        "colab_type": "code",
        "outputId": "0610fade-0c09-4f67-f04b-916688e43cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "errs = np.array(sd)\n",
        "\n",
        "p = 0.2 # fraction of noise\n",
        "errs = 1.0 - (1-p)*(1-errs) + errs*p/9 # remap clean test error --> noisy test error.\n",
        "\n",
        "stds = np.std(errs, axis=0)\n",
        "mean = np.mean(errs, axis=0)\n",
        "#train_err = np.mean([M['Train Error'][:, -1] for M in Ms], axis=0)\n",
        "\n",
        "ks=np.array([M for M in range(1,100)])\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(ks,sd , label='Test Error')\n",
        "#ax.plot(ks, train_err, label='Train Error')\n",
        "#ax.fill_between(ks, mean-stds, mean+stds ,alpha=0.3)\n",
        "ax.set_xlabel(\"Model Size\")\n",
        "ax.set_ylabel(\"Test/Train Error\")\n",
        "ax.set_title(\"ResNet18 Double-Descent\")\n",
        "ax.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe39c7f24a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGvotOT84fJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inWlr7y_o0g4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = fetch_openml(data_id=40926, return_X_y=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iAve6DCL4JH4",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRsylInMXdxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss={}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-D_80hwTy7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.signal\n",
        "import matplotlib.pyplot as plt\n",
        "x=[]\n",
        "x1=[]\n",
        "y1=[]\n",
        "y=[]\n",
        "x2=[]\n",
        "y2=[]\n",
        "#for i,v in enumerate(test_err6):\n",
        " # x2.append(i+1)\n",
        "  #y2.append(v)\n",
        "#for i,v in enumerate(test_err5):\n",
        " # x.append(i+1)\n",
        "  #y.append(v)\n",
        "for i,v in tent.items():\n",
        "  x1.append(i)\n",
        "  y1.append(v)\n",
        "#yhat = scipy.signal.savgol_filter(y, 21, 1)\n",
        "yhat1 = scipy.signal.savgol_filter(yt, 5, 2)\n",
        "yhat2 = scipy.signal.savgol_filter(yq, 5, 3)\n",
        "#th_dict=[]\n",
        "#te=[]\n",
        "fig, ax = plt.subplots()\n",
        "#ite =1\n",
        "#for ind,val in enumerate(results.history['val_accuracy']):\n",
        " # th_dict[ite] = val\n",
        "  #te.append(1-val)\n",
        "  #if ind== 100:\n",
        "   # ite+=1\n",
        "#plt.plot(ks, errs, label='Test Error')\n",
        "ax.axvspan(21, 28, alpha=0.5, color='Red')\n",
        "plt.plot(xt,yhat1,label='Resnetv2-20 5000 Samples')\n",
        "plt.plot(xq,yhat2,label='Resnetv2-20 25000 Samples')\n",
        "\n",
        "#plt.plot(x,yhat,label='Resnetv2-20 Parameter Width= 64')\n",
        "#plt.annotate('Interpolation Threshold', xy=(28,0.3), xytext=(28, 0.35),\n",
        " #            arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "  #           )\n",
        "plt.xlim([0,64])\n",
        "plt.xlabel('Model Width Parameter')\n",
        "plt.ylabel('Test Error')\n",
        "plt.legend(loc='upper right')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEoDmQvKRqRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "test_error={}\n",
        "te =[]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrFAHQ1CSZ7x",
        "colab_type": "code",
        "outputId": "8b5a6042-70b6-4762-c299-916b8ebf612a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, GaussianNoise\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
        "epochs = 100\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# Model parameter\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 2\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 2\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "#x_train=x_train[:12500]\n",
        "#y_train=y_train[:12500]\n",
        "#x_test=x_test[:2500]\n",
        "#y_test=y_test[:2500]\n",
        "#print(len(y_train))\n",
        "#(x_train, y_train)=new_dataset[0.8:0.2]\n",
        "#(x_test, y_test) =newtest_dataset\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "def add_noise_and_clip_data(data):\n",
        "   noise = np.random.normal(loc=0.0, scale=0.4, size=data.shape)\n",
        "\n",
        "   data = data + noise\n",
        "   data = np.clip(data, 0., 1.)\n",
        "   return data\n",
        "\n",
        "\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "#y_train = add_noise_and_clip_data(y_train)\n",
        "#y_test = add_noise_and_clip_data(y_test)\n",
        "\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "   # if epoch > 180:\n",
        "    #    lr *= 0.5e-3\n",
        "   # elif epoch > 160:\n",
        "    #    lr *= 1e-3\n",
        "    #elif epoch > 120:\n",
        "    #    lr *= 1e-2\n",
        "    #elif epoch > 100:\n",
        "     #   lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal')\n",
        "                  #kernel_regularizer=l2(5e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        x = GaussianNoise(2)(x)\n",
        "        if batch_normalization:\n",
        "\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(k,input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = k#16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet_v2(k,input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = k\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "arr=[64]#1,8,16,24,32,40,48,54,64]\n",
        "te=[]\n",
        "\n",
        "for k in arr:\n",
        "    if version == 2:\n",
        "        model = resnet_v2(k,input_shape=input_shape, depth=depth)\n",
        "    else:\n",
        "        model = resnet_v1(k,input_shape=input_shape, depth=depth)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "    model.summary()\n",
        "  #print(model[k]_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "    save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "    model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "    checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                               monitor='val_acc',\n",
        "                               verbose=1,\n",
        "                               save_best_only=True)\n",
        "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "    lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "  \n",
        "    callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "    if not data_augmentation:\n",
        "        print('Not using data augmentation.')\n",
        "        history = model.fit(x_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs,\n",
        "                validation_data=(x_test, y_test),\n",
        "                shuffle=True,\n",
        "                callbacks=callbacks)\n",
        "    else:\n",
        "        print('Using real-time data augmentation.')\n",
        "      # This will do preprocessing and realtime data augmentation:\n",
        "        datagen = ImageDataGenerator(\n",
        "          # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history= model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks)\n",
        "   \n",
        "\n",
        "    for t in history.history['val_accuracy']:\n",
        "        te.append(1-t)\n",
        "\n",
        "    plt.plot(te, label = k)\n",
        "    plt.xlim([0,epochs])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Test Error')\n",
        "    plt.legend(loc='upper right')\n",
        "  \n",
        "# Score trained model.\n",
        "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "    print('k Test loss:', scores[0])\n",
        "    test_loss[k]=scores[0]\n",
        "    print('k Test accuracy:', scores[1])\n",
        "    test_error[k]=1-scores[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n",
            "Learning rate:  0.001\n",
            "Model: \"model_19\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_23 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1129 (Conv2D)            (None, 32, 32, 64)   1792        input_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "gaussian_noise_10 (GaussianNois (None, 32, 32, 64)   0           conv2d_1129[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1064 (Batch (None, 32, 32, 64)   256         gaussian_noise_10[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_1075 (Activation)    (None, 32, 32, 64)   0           batch_normalization_1064[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1130 (Conv2D)            (None, 32, 32, 64)   4160        activation_1075[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1065 (Batch (None, 32, 32, 64)   256         conv2d_1130[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1076 (Activation)    (None, 32, 32, 64)   0           batch_normalization_1065[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1131 (Conv2D)            (None, 32, 32, 64)   36928       activation_1076[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1066 (Batch (None, 32, 32, 64)   256         conv2d_1131[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1133 (Conv2D)            (None, 32, 32, 256)  16640       activation_1075[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1077 (Activation)    (None, 32, 32, 64)   0           batch_normalization_1066[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "gaussian_noise_11 (GaussianNois (None, 32, 32, 256)  0           conv2d_1133[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1132 (Conv2D)            (None, 32, 32, 256)  16640       activation_1077[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_349 (Add)                   (None, 32, 32, 256)  0           gaussian_noise_11[0][0]          \n",
            "                                                                 conv2d_1132[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1067 (Batch (None, 32, 32, 256)  1024        add_349[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1078 (Activation)    (None, 32, 32, 256)  0           batch_normalization_1067[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1134 (Conv2D)            (None, 32, 32, 64)   16448       activation_1078[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1068 (Batch (None, 32, 32, 64)   256         conv2d_1134[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1079 (Activation)    (None, 32, 32, 64)   0           batch_normalization_1068[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1135 (Conv2D)            (None, 32, 32, 64)   36928       activation_1079[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1069 (Batch (None, 32, 32, 64)   256         conv2d_1135[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1080 (Activation)    (None, 32, 32, 64)   0           batch_normalization_1069[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1136 (Conv2D)            (None, 32, 32, 256)  16640       activation_1080[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_350 (Add)                   (None, 32, 32, 256)  0           add_349[0][0]                    \n",
            "                                                                 conv2d_1136[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1070 (Batch (None, 32, 32, 256)  1024        add_350[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1081 (Activation)    (None, 32, 32, 256)  0           batch_normalization_1070[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1137 (Conv2D)            (None, 16, 16, 256)  65792       activation_1081[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1071 (Batch (None, 16, 16, 256)  1024        conv2d_1137[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1082 (Activation)    (None, 16, 16, 256)  0           batch_normalization_1071[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1138 (Conv2D)            (None, 16, 16, 256)  590080      activation_1082[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1072 (Batch (None, 16, 16, 256)  1024        conv2d_1138[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1140 (Conv2D)            (None, 16, 16, 512)  131584      add_350[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1083 (Activation)    (None, 16, 16, 256)  0           batch_normalization_1072[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "gaussian_noise_12 (GaussianNois (None, 16, 16, 512)  0           conv2d_1140[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1139 (Conv2D)            (None, 16, 16, 512)  131584      activation_1083[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_351 (Add)                   (None, 16, 16, 512)  0           gaussian_noise_12[0][0]          \n",
            "                                                                 conv2d_1139[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1073 (Batch (None, 16, 16, 512)  2048        add_351[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1084 (Activation)    (None, 16, 16, 512)  0           batch_normalization_1073[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1141 (Conv2D)            (None, 16, 16, 256)  131328      activation_1084[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1074 (Batch (None, 16, 16, 256)  1024        conv2d_1141[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1085 (Activation)    (None, 16, 16, 256)  0           batch_normalization_1074[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1142 (Conv2D)            (None, 16, 16, 256)  590080      activation_1085[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1075 (Batch (None, 16, 16, 256)  1024        conv2d_1142[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1086 (Activation)    (None, 16, 16, 256)  0           batch_normalization_1075[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1143 (Conv2D)            (None, 16, 16, 512)  131584      activation_1086[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_352 (Add)                   (None, 16, 16, 512)  0           add_351[0][0]                    \n",
            "                                                                 conv2d_1143[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1076 (Batch (None, 16, 16, 512)  2048        add_352[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1087 (Activation)    (None, 16, 16, 512)  0           batch_normalization_1076[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1144 (Conv2D)            (None, 8, 8, 512)    262656      activation_1087[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1077 (Batch (None, 8, 8, 512)    2048        conv2d_1144[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1088 (Activation)    (None, 8, 8, 512)    0           batch_normalization_1077[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1145 (Conv2D)            (None, 8, 8, 512)    2359808     activation_1088[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1078 (Batch (None, 8, 8, 512)    2048        conv2d_1145[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1147 (Conv2D)            (None, 8, 8, 1024)   525312      add_352[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1089 (Activation)    (None, 8, 8, 512)    0           batch_normalization_1078[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "gaussian_noise_13 (GaussianNois (None, 8, 8, 1024)   0           conv2d_1147[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1146 (Conv2D)            (None, 8, 8, 1024)   525312      activation_1089[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_353 (Add)                   (None, 8, 8, 1024)   0           gaussian_noise_13[0][0]          \n",
            "                                                                 conv2d_1146[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1079 (Batch (None, 8, 8, 1024)   4096        add_353[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1090 (Activation)    (None, 8, 8, 1024)   0           batch_normalization_1079[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1148 (Conv2D)            (None, 8, 8, 512)    524800      activation_1090[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1080 (Batch (None, 8, 8, 512)    2048        conv2d_1148[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1091 (Activation)    (None, 8, 8, 512)    0           batch_normalization_1080[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1149 (Conv2D)            (None, 8, 8, 512)    2359808     activation_1091[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1081 (Batch (None, 8, 8, 512)    2048        conv2d_1149[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1092 (Activation)    (None, 8, 8, 512)    0           batch_normalization_1081[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1150 (Conv2D)            (None, 8, 8, 1024)   525312      activation_1092[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_354 (Add)                   (None, 8, 8, 1024)   0           add_353[0][0]                    \n",
            "                                                                 conv2d_1150[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1082 (Batch (None, 8, 8, 1024)   4096        add_354[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1093 (Activation)    (None, 8, 8, 1024)   0           batch_normalization_1082[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_19 (AveragePo (None, 1, 1, 1024)   0           activation_1093[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_21 (Flatten)            (None, 1024)         0           average_pooling2d_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 10)           10250       flatten_21[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 9,039,370\n",
            "Trainable params: 9,025,418\n",
            "Non-trainable params: 13,952\n",
            "__________________________________________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 98s 249ms/step - loss: 1.9001 - accuracy: 0.2991 - val_loss: 4.9806 - val_accuracy: 0.1595\n",
            "Epoch 2/100\n",
            "Learning rate:  0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 93s 237ms/step - loss: 1.5675 - accuracy: 0.4245 - val_loss: 2.4197 - val_accuracy: 0.2765\n",
            "Epoch 3/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 1.4005 - accuracy: 0.4897 - val_loss: 1.8985 - val_accuracy: 0.4116\n",
            "Epoch 4/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 1.2686 - accuracy: 0.5418 - val_loss: 1.7521 - val_accuracy: 0.5014\n",
            "Epoch 5/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 1.1539 - accuracy: 0.5861 - val_loss: 1.6040 - val_accuracy: 0.5144\n",
            "Epoch 6/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 1.0515 - accuracy: 0.6249 - val_loss: 1.1777 - val_accuracy: 0.6058\n",
            "Epoch 7/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.9637 - accuracy: 0.6572 - val_loss: 1.2038 - val_accuracy: 0.6374\n",
            "Epoch 8/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.8934 - accuracy: 0.6836 - val_loss: 1.1826 - val_accuracy: 0.6445\n",
            "Epoch 9/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.8371 - accuracy: 0.7067 - val_loss: 1.1269 - val_accuracy: 0.6563\n",
            "Epoch 10/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.7818 - accuracy: 0.7264 - val_loss: 0.9396 - val_accuracy: 0.7080\n",
            "Epoch 11/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.7339 - accuracy: 0.7417 - val_loss: 1.0623 - val_accuracy: 0.6875\n",
            "Epoch 12/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.7005 - accuracy: 0.7539 - val_loss: 1.1975 - val_accuracy: 0.6506\n",
            "Epoch 13/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.6696 - accuracy: 0.7654 - val_loss: 0.7912 - val_accuracy: 0.7462\n",
            "Epoch 14/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.6432 - accuracy: 0.7742 - val_loss: 0.9817 - val_accuracy: 0.7054\n",
            "Epoch 15/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.6059 - accuracy: 0.7869 - val_loss: 0.9167 - val_accuracy: 0.7264\n",
            "Epoch 16/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.5870 - accuracy: 0.7943 - val_loss: 0.8112 - val_accuracy: 0.7413\n",
            "Epoch 17/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.5603 - accuracy: 0.8040 - val_loss: 0.8109 - val_accuracy: 0.7623\n",
            "Epoch 18/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.5412 - accuracy: 0.8100 - val_loss: 0.7278 - val_accuracy: 0.7764\n",
            "Epoch 19/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.5193 - accuracy: 0.8172 - val_loss: 0.6404 - val_accuracy: 0.7910\n",
            "Epoch 20/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.4969 - accuracy: 0.8273 - val_loss: 0.6666 - val_accuracy: 0.8010\n",
            "Epoch 21/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.4816 - accuracy: 0.8314 - val_loss: 0.6909 - val_accuracy: 0.7817\n",
            "Epoch 22/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.4629 - accuracy: 0.8392 - val_loss: 1.0080 - val_accuracy: 0.7254\n",
            "Epoch 23/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.4487 - accuracy: 0.8441 - val_loss: 0.6675 - val_accuracy: 0.7993\n",
            "Epoch 24/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.4302 - accuracy: 0.8483 - val_loss: 0.5432 - val_accuracy: 0.8280\n",
            "Epoch 25/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.4165 - accuracy: 0.8526 - val_loss: 0.7648 - val_accuracy: 0.7849\n",
            "Epoch 26/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.4029 - accuracy: 0.8588 - val_loss: 0.6183 - val_accuracy: 0.8085\n",
            "Epoch 27/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.3880 - accuracy: 0.8629 - val_loss: 0.6466 - val_accuracy: 0.8077\n",
            "Epoch 28/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.3695 - accuracy: 0.8685 - val_loss: 0.7124 - val_accuracy: 0.7896\n",
            "Epoch 29/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.3588 - accuracy: 0.8725 - val_loss: 0.6377 - val_accuracy: 0.8088\n",
            "Epoch 30/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.3508 - accuracy: 0.8760 - val_loss: 0.8371 - val_accuracy: 0.7806\n",
            "Epoch 31/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 237ms/step - loss: 0.3314 - accuracy: 0.8828 - val_loss: 0.5404 - val_accuracy: 0.8405\n",
            "Epoch 32/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.3262 - accuracy: 0.8846 - val_loss: 0.5628 - val_accuracy: 0.8307\n",
            "Epoch 33/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.3132 - accuracy: 0.8897 - val_loss: 0.7031 - val_accuracy: 0.8120\n",
            "Epoch 34/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.3002 - accuracy: 0.8949 - val_loss: 0.5861 - val_accuracy: 0.8373\n",
            "Epoch 35/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.2867 - accuracy: 0.8991 - val_loss: 0.5597 - val_accuracy: 0.8402\n",
            "Epoch 36/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.2773 - accuracy: 0.9009 - val_loss: 0.5919 - val_accuracy: 0.8305\n",
            "Epoch 37/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.2677 - accuracy: 0.9058 - val_loss: 0.6058 - val_accuracy: 0.8295\n",
            "Epoch 38/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.2621 - accuracy: 0.9081 - val_loss: 0.6082 - val_accuracy: 0.8322\n",
            "Epoch 39/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.2502 - accuracy: 0.9102 - val_loss: 0.5522 - val_accuracy: 0.8482\n",
            "Epoch 40/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.2416 - accuracy: 0.9143 - val_loss: 0.6474 - val_accuracy: 0.8289\n",
            "Epoch 41/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.2316 - accuracy: 0.9177 - val_loss: 0.6952 - val_accuracy: 0.8189\n",
            "Epoch 42/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.2247 - accuracy: 0.9194 - val_loss: 0.5210 - val_accuracy: 0.8508\n",
            "Epoch 43/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.2186 - accuracy: 0.9213 - val_loss: 0.5871 - val_accuracy: 0.8442\n",
            "Epoch 44/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.2116 - accuracy: 0.9245 - val_loss: 0.5250 - val_accuracy: 0.8630\n",
            "Epoch 45/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.2024 - accuracy: 0.9271 - val_loss: 0.6056 - val_accuracy: 0.8480\n",
            "Epoch 46/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1954 - accuracy: 0.9289 - val_loss: 0.5045 - val_accuracy: 0.8620\n",
            "Epoch 47/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1847 - accuracy: 0.9330 - val_loss: 0.5978 - val_accuracy: 0.8478\n",
            "Epoch 48/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1777 - accuracy: 0.9361 - val_loss: 0.6011 - val_accuracy: 0.8535\n",
            "Epoch 49/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1718 - accuracy: 0.9387 - val_loss: 0.6232 - val_accuracy: 0.8433\n",
            "Epoch 50/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1696 - accuracy: 0.9394 - val_loss: 0.6172 - val_accuracy: 0.8497\n",
            "Epoch 51/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1612 - accuracy: 0.9420 - val_loss: 0.6234 - val_accuracy: 0.8449\n",
            "Epoch 52/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1544 - accuracy: 0.9440 - val_loss: 0.5744 - val_accuracy: 0.8486\n",
            "Epoch 53/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1495 - accuracy: 0.9453 - val_loss: 0.6873 - val_accuracy: 0.8406\n",
            "Epoch 54/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1473 - accuracy: 0.9472 - val_loss: 0.6916 - val_accuracy: 0.8405\n",
            "Epoch 55/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 239ms/step - loss: 0.1421 - accuracy: 0.9486 - val_loss: 0.6668 - val_accuracy: 0.8431\n",
            "Epoch 56/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1388 - accuracy: 0.9503 - val_loss: 0.6385 - val_accuracy: 0.8490\n",
            "Epoch 57/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1275 - accuracy: 0.9542 - val_loss: 0.5511 - val_accuracy: 0.8685\n",
            "Epoch 58/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1248 - accuracy: 0.9559 - val_loss: 0.6129 - val_accuracy: 0.8559\n",
            "Epoch 59/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1226 - accuracy: 0.9547 - val_loss: 0.6288 - val_accuracy: 0.8527\n",
            "Epoch 60/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1159 - accuracy: 0.9590 - val_loss: 0.6546 - val_accuracy: 0.8547\n",
            "Epoch 61/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1165 - accuracy: 0.9586 - val_loss: 0.6075 - val_accuracy: 0.8571\n",
            "Epoch 62/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1142 - accuracy: 0.9586 - val_loss: 0.6370 - val_accuracy: 0.8548\n",
            "Epoch 63/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1100 - accuracy: 0.9607 - val_loss: 0.5770 - val_accuracy: 0.8661\n",
            "Epoch 64/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.1091 - accuracy: 0.9601 - val_loss: 0.7914 - val_accuracy: 0.8436\n",
            "Epoch 65/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 239ms/step - loss: 0.1015 - accuracy: 0.9638 - val_loss: 0.6162 - val_accuracy: 0.8597\n",
            "Epoch 66/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 239ms/step - loss: 0.0972 - accuracy: 0.9661 - val_loss: 0.7785 - val_accuracy: 0.8377\n",
            "Epoch 67/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.0947 - accuracy: 0.9658 - val_loss: 0.6625 - val_accuracy: 0.8499\n",
            "Epoch 68/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 239ms/step - loss: 0.0998 - accuracy: 0.9639 - val_loss: 0.6094 - val_accuracy: 0.8612\n",
            "Epoch 69/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 239ms/step - loss: 0.0906 - accuracy: 0.9683 - val_loss: 0.6738 - val_accuracy: 0.8587\n",
            "Epoch 70/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.0865 - accuracy: 0.9692 - val_loss: 0.6568 - val_accuracy: 0.8616\n",
            "Epoch 71/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.0809 - accuracy: 0.9711 - val_loss: 0.6528 - val_accuracy: 0.8586\n",
            "Epoch 72/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.0885 - accuracy: 0.9683 - val_loss: 0.6675 - val_accuracy: 0.8658\n",
            "Epoch 73/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.0784 - accuracy: 0.9717 - val_loss: 0.6495 - val_accuracy: 0.8682\n",
            "Epoch 74/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.0847 - accuracy: 0.9701 - val_loss: 0.7526 - val_accuracy: 0.8564\n",
            "Epoch 75/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.0766 - accuracy: 0.9724 - val_loss: 0.5987 - val_accuracy: 0.8657\n",
            "Epoch 76/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.0817 - accuracy: 0.9708 - val_loss: 0.6251 - val_accuracy: 0.8692\n",
            "Epoch 77/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 93s 238ms/step - loss: 0.0747 - accuracy: 0.9734 - val_loss: 0.6575 - val_accuracy: 0.8608\n",
            "Epoch 78/100\n",
            "Learning rate:  0.001\n",
            "224/391 [================>.............] - ETA: 37s - loss: 0.0711 - accuracy: 0.9746"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc_4R7YpE214",
        "colab_type": "code",
        "outputId": "65db6971-78c6-4351-c0bd-4d96b7eb471b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "plt.plot(sd)\n",
        "plt.xlim([0,epochs])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Error')\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe398abae80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOy9eXxcd3nv/3lm1yzSaLW1WZIdO46X2E4cx0kgCYUkBn4kgVAItBB6gZC2KbRQtt4WuKEt5ZYLlDYUAqTAqw0BwlJTAiEhZCWL5XiLd1u2JVn7MiNp9uX5/XHOmTkzc2Y0M9JIsvy8Xy+9rDlzzpyvZ0bnc56dmBmCIAiCUA6mxV6AIAiCcOEiIiIIgiCUjYiIIAiCUDYiIoIgCELZiIgIgiAIZWNZ7AXMFw0NDdzZ2bnYyxAEQbig2Lt37xgzN5Z7/LIRkc7OTnR3dy/2MgRBEC4oiOjcXI4Xd5YgCIJQNiIigiAIQtmIiAiCIAhls2xiIoIgCBc7sVgM/f39CIfDOc85HA60tbXBarXO6zlFRARBEJYJ/f398Hg86OzsBBGltjMzxsfH0d/fj66urnk9p7izBEEQlgnhcBj19fUZAgIARIT6+npDC2WuiIgIgiAsI7IFZLbtc6WiIkJEu4joOBGdIqJPGTz/FSLar/6cICKf7rm7iOik+nPXbOcangpjdDoy3/8FQRAEoQAVi4kQkRnA/QBuAtAPYA8R7WbmI9o+zPxXuv3/AsA29fc6AJ8FsB0AA9irHjuZ73wj0xH0TgTQ6LFX5P8jCIIg5FJJS2QHgFPM3MPMUQAPA7itwP7vAvAD9fdbADzOzBOqcDwOYNdsJxRLRBCEi518gwYrNYCwkiLSCqBP97hf3ZYDEXUA6ALwZCnHEtHdRNRNRN2AiIggCBc3DocD4+PjOYKhZWc5HI55P+dSSfG9E8AjzJwo5SBmfgDAAwBgb17LozPRSqxNEAThgqCtrQ39/f0YHR3NeU6rE5lvKiki5wG06x63qduMuBPAn2cde2PWsU8VOpnFRGKJCIJwUWO1Wue9DmQ2KunO2gNgLRF1EZENilDszt6JiNYDqAXwgm7zYwBuJqJaIqoFcLO6LS8Wk0lERBAEYYGpmCXCzHEiuhfKxd8M4EFmPkxE9wHoZmZNUO4E8DDrnHjMPEFEn4ciRABwHzNPFDqfxUwYnREREQRBWEgqGhNh5kcBPJq17TNZjz+X59gHATxY7LksZsKYWCKCIAgLyrKpWLeaTBidiVQsjU0QBEHIZdmIiMVMiMaTmArHF3spgiAIFw3LR0RMyn9lTOIigiAIC8byERGz0lxMMrQEQRAWjuUjIqolIiIiCIKwcCwfERFLRBAEYcFZPiJiIlhMJDERQRCEBWTZiAgANLjtYokIgiAsIMtKRBo9dqlaFwRBWECWlYg0uG1iiQiCICwgy0pEGj12iYkIgiAsIMtQRKJIJqX1iSAIwkKwvETEbUciyZgMynAqQRCEhWBZiUiDxw4AElwXBEFYIJaViDS6FREZmxZLRBAEYSFYXiKSskTCi7wSQRCEi4PlKSKS5isIgrAgLCsRcdstsFtk1rogCMJCUVERIaJdRHSciE4R0afy7PMOIjpCRIeJ6CHd9gQR7Vd/dhsda/BaqTRfQRAEofJUbMY6EZkB3A/gJgD9APYQ0W5mPqLbZy2ATwO4jpkniahJ9xIhZt5a6nkbPdI/SxAEYaGopCWyA8ApZu5h5iiAhwHclrXPBwHcz8yTAMDMI3M9qTRhFARBWDgqKSKtAPp0j/vVbXrWAVhHRM8T0YtEtEv3nIOIutXttxudgIjuVvfpHh0dBSCtTwRBEBaSirmzSjj/WgA3AmgD8AwRbWZmH4AOZj5PRKsBPElEh5j5tP5gZn4AwAMAsH37dgaUWpGJYBSxRBJW87LKGxAEQVhyVPIqex5Au+5xm7pNTz+A3cwcY+YzAE5AERUw83n13x4ATwHYVsxJGz12MAMTAQmuC4IgVJpKisgeAGuJqIuIbADuBJCdZfVzKFYIiKgBinurh4hqiciu234dgCMogga31IoIgiAsFBVzZzFznIjuBfAYADOAB5n5MBHdB6CbmXerz91MREcAJAB8nJnHiehaAN8koiQUofsnfVZXIRqlf5YgCMKCUdGYCDM/CuDRrG2f0f3OAD6q/uj3+T2AzeWcs0mq1gVBEBaMZRd5FneWIAjCwrHsRKTKZobbbpE0X0EQhAVg2YkIAHidVviCscVehiAIwrJnWYpIncsmKb6CIAgLwLIUEa/TBp+MyBUEQag4y1JE6pxWTIiICIIgVJxlKSJepw2+gMREBEEQKs2yFJE6lw3TkTii8eRiL0UQBGFZsyxFpNZpBQD4QuLSEgRBqCTLU0RcNgCQNF9BEIQKszxFxKmIiKT5CoIgVJZlKSJezZ0lGVqCIAgVZVmKSJ1Ls0TEnSUIglBJlqWIaO6sSbFEBEEQKsqyFBGH1YwqqxmTEhMRBEGoKMtSRAAlzXdSsrMEQRAqyvIVEZdN3FmCIAgVZvmKiFNERBAEodJUVESIaBcRHSeiU0T0qTz7vIOIjhDRYSJ6SLf9LiI6qf7cVeq5a102iYkIgiBUmIrNWCciM4D7AdwEoB/AHiLazcxHdPusBfBpANcx8yQRNanb6wB8FsB2AAxgr3rsZLHnl5iIIAhC5amkJbIDwClm7mHmKICHAdyWtc8HAdyviQMzj6jbbwHwODNPqM89DmBXKSf3Om2YCscQT0gTRkEQhEpRSRFpBdCne9yvbtOzDsA6InqeiF4kol0lHAsiupuIuomoe3R0NOO5OqcVzIA/JNaIIAhCpVjswLoFwFoANwJ4F4BvEZG32IOZ+QFm3s7M2xsbGzOe05owiktLEAShclRSRM4DaNc9blO36ekHsJuZY8x8BsAJKKJSzLEFkap1QRCEylNJEdkDYC0RdRGRDcCdAHZn7fNzKFYIiKgBinurB8BjAG4moloiqgVws7qtaFIiIhlagiAIFaNi2VnMHCeie6Fc/M0AHmTmw0R0H4BuZt6NtFgcAZAA8HFmHgcAIvo8FCECgPuYeaKU89e6lE6+YokIgiBUjoqJCAAw86MAHs3a9hnd7wzgo+pP9rEPAniw3HOn3VkSExEEQagUix1YrxhOmxk2i0ncWYIgCBVk2YoIEakFhyIigiAIlWLZigiguLRkMJUgCELlWPYiIiNyBUEQKsfyFhGXuLMEQRAqyfIWEadNsrMEQRAqyLIXEV8wimSSF3spgiAIy5LlLSIuG5IMTIXFGhEEQagEy1tEnFrVuoiIIAhCJVjeIqJ28p2QgkNBEISKsLxFRG19Imm+giAIlWFZi0idUywRQRCESrKsRcSrdvL1SUxEEAShIixrEfHYLbCYCBPizhIEQagIy1pEiAheaX0iCIJQMQqKCBGZieh3C7WYSlDrtGJSmjAKgiBUhIIiwswJAEkiqlmg9cw7tS6buLMEQRAqRDGTDWcAHCKixwEEtI3M/OGKrWoeqXVacWYsMPuOgiAIQskUExP5KYC/A/AMgL26n1khol1EdJyIThHRpwyefx8RjRLRfvXnA7rnErrtu4v77+TSXFOFvokQZiLxcl9CEARByMOslggzf4+IbADWqZuOM/OsQQYiMgO4H8BNAPoB7CGi3cx8JGvXHzLzvQYvEWLmrbOdZzbesqUF3/39WezeP4B3X71qri8nCIIg6JjVEiGiGwGchCIIXwdwgoiuL+K1dwA4xcw9zBwF8DCA2+aw1rK4YpUX61d68J8vngOzdPMVBEGYT4pxZ/0/ADcz8w3MfD2AWwB8pYjjWgH06R73q9uyuYOIDhLRI0TUrtvuIKJuInqRiG43OgER3a3u0z06Omq4CCLCH+/swJHBKezv8xWxbEEQBKFYihERKzMf1x4w8wkA1nk6/y8AdDLz5QAeB/A93XMdzLwdwLsBfJWI1mQfzMwPMPN2Zt7e2NiY9yS3b2uFy2bGf77Ym3efIX8YL/aMl/v/EARBuCgpRkT2EtG3iehG9edbALqLOO48AL1l0aZuS8HM48wcUR9+G8CVuufOq//2AHgKwLYizmmI227B7dta8T8HB/IWHn5u92Hc9eDLiCWS5Z5GEAThoqMYEbkHwBEAH1Z/jgD40yKO2wNgLRF1qYH5OwFkZFkRUbPu4a0Ajqrba4nIrv7eAOA69bxl80dXdyAST+Inr5zPec4fjOHJYyOIxJPoGZV0YEEQhGIpmJ2lZlgdYOb1AL5cygszc5yI7gXwGAAzgAeZ+TAR3Qegm5l3A/gwEd0KIA5gAsD71MMvA/BNIkpCEbp/MsjqKokNLdW4YpUX//XSOfyv6zpBRKnnfnloEFHVAjk84MelKz1zOZUgCMJFQzEV68eJqKzcWGZ+lJnXMfMaZv4HddtnVAEBM3+amTcy8xZmfh0zH1O3/56ZN6vbNzPzd8o5fzZ/dHUHekYDePLYSMb2n+87j9WNLtgtJhwZmJqPUwmCIFwUFOPOqgVwmIh+S0S7tZ9KL6wSvGVLCzrrnfjCr44hrloefRNBvHx2Andc0Yb1Kz04MigiIgiCUCzFtD35u4qvYoGwWUz41Bsvwz3/uRcP7+nDH+/swO4DAwCAW7e0oH8yiF+9OgRmznB3CYIgCMbM2sUXwDeZ+ensnwVa37xzy8YV2NFZh688fgLT4Rh++ko/dnTVob3OiQ3N1fAFYxj0hxd7mYIgCBcEFY2JLEWICH/7/12G8UAUH3l4P06PBvDWbUoN5IaWagCQuIggCEKRFOPO0mIiLyOzi++tFVtVhbm8zYvbt7bg5/sHYDOb8KZNSqbx+pXVIAKODE7hDRtWLPIqgUg8AWbAYTUv9lIEQRAMuahiIno+vms9fvXqEP5gfRNqnEoBvstuQVe9a8lYIp945CDCsQS++Z7ti70UQRAEQ/KKCBGtZ+ZjzPw0Edl1leUgop0Ls7zK0eqtws/+7Do0Vdsztl/WUo1D/f5FWlUmZ8eDiMQSi70MQRCEvBSKiTyk+/2FrOe+XoG1LDgbWqrR4M4UkQ3N1eidCGIqvPgjdWfCMZmDIgjCkqaQiFCe340eLxu04PrRObq0kklGKDo3KyIQSYiICIKwpCkkIpznd6PHy4aNzWqG1hyLDn/U3YfrvvgkovHyGzrOROKYCcdlDoogCEuWQoH1NiL6GhSrQ/sd6mOjuSDLgkaPHQ1u25yD64cHpjARiMIXjKKp2lHy8ckkp6yQSDwpGVqCICxJConIx3W/Z7d+L6YV/AUJEWFDS82cLZEBXwgAMBmMlSUigWjajTUdjouICIKwJMkrIsz8vXzPLXc2NFfjwefOIBpPwmYppr1YLgNq1ftknvkls6GPhcxE4mj02AvsLQiCsDiUd4Vc5mxoqUY0kcSJ4emyX2PQr1gi+YZgzcZMOG74uyAIwlJCRMSAHZ11cFhN+KdfHUMyWXpQOxiNwxdUUoQng+WlCk/rLJHpyOKnGwuCIBgxq4gQ0XXFbFtOrKxx4LNv2YjnTo3hgWd7Sj5+wJdu4FiuOysQEUtEEISlTzGWyL8WuW1ZcedV7Xjz5mZ86bHj2Nc7WdKxmisLQMoiKZUMd5bUigiCsEQp1PbkGgDXAmgkoo/qnqqGMu52WUNE+Me3bcb+Ph8+/PA+/PLDr0W1w1rUsYOqJWI2ESYC5Vki0xEREUEQlj6FLBEbADcUofHofqYAvL2YFyeiXUR0nIhOEdGnDJ5/HxGNEtF+9ecDuufuIqKT6s9dpfyn5ouaKiu+9q6tGPCF8clHDhYdHznvC4EI6GpwzUtgfVrcWYIgLFEKpfg+DeBpIvouM58DACIyAXAz86xFFOpAq/sB3ASgH8AeItrNzEeydv0hM9+bdWwdgM8C2A6lOn6vemxpfqV54MqOOnz6jevx9788iq88cQIfu/nSWY8Z9IfQ4LajyWMvO7CuWR9mE4klIgjCkqWYmMgXiKiaiFwAXgVwhIg+PttBAHYAOMXMPcwcBfAwgNuKXNctAB5n5glVOB4HsKvIY+ed97+mC+/c3o5/ffIUfr7v/Kz7D/rDaPFWodZpm1OdiMNqQk2VVQLrgiAsWYoRkQ2q5XE7gF8B6ALwniKOawXQp3vcD+N2KXcQ0UEieoSI2ks5lojuJqJuIuoeHR0tYknlQUT4/O2bcHVXHT7xk4PYe66wQXTeF0JLjQNep7X8wHokDrfdArfdIpaIIAhLlmJExEpEVigispuZY5i/Boy/ANDJzJdDsTZKqpJn5geYeTszb29sbJynJRljs5jwjT++Ei01Dtz70Ct5myIyMwZ9YTTXKJaILxgtq9ZkJpwWEYmJCIKwVClGRL4J4CwAF4BniKgDSnB9Ns4DaNc9blO3pWDmcd2wq28DuLLYYxeDWpcN772mE4P+cN5Yhz8UQyiWQItXsUSSXF5gfCYSh9thgdthwYwUGwqCsESZVUSY+WvM3MrMb2KFcwBeV8Rr7wGwloi6iMgG4E4Au/U7EFGz7uGtAI6qvz8G4GYiqiWiWgA3q9sWnRav0kxRa7CYjVZoqMVEgPIKDjVLxCPurIpzamQGb//332N6CQwiE4QLjWIq1lcQ0XeI6Ffq4w0AZk25ZeY4gHuhXPyPAvgRMx8movuI6FZ1tw8T0WEiOgDgwwDepx47AeDzUIRoD4D71G2LzsqaKgBK8NwITVyaaxyodSl1JeWIyHQkDrfdqlgi4s6qKHvPTaD73CROjwYWeymCcMFRqBW8xncB/AeA/60+PgHghwC+M9uBzPwogEeztn1G9/unAXw6z7EPAniwiPUtKC01iiWir0rXo21v9ValAkflBNdnIjF4HB44bWaxRCrMREDrc1ZeJp0gXMzktUSISBOYBmb+EYAkkLIw5jb39QKmwW2H1Uz5LRF/GFYzocFtnxd3ltshgfVKoxWETpbZXUAQLmYKubNeVv8NEFE91IwsItoJwF/phS1VTCbCimoHBvPGREJYUe2AyUSodWrurNItkUAkAZcaE4nEk3MasysURhP5cgtDBeFippA7i9R/PwolIL6GiJ4H0Igi254sV1pqqlJDp7IZ9CmFhgBQ7bDCRKXPFInEE4gmkvA4LHDZlDZlgUgcNottbgsXDNHEo9wWNYJwMVNIRPSNF38GJbZBACIA3gDgYIXXtmRp9jrwSp7OvgP+ELZ31AJQrJaaKmvJ7iwtkO62W+CyKx/RTCSOWpeISCXQ3FgSExGE0ikkImYoDRgpa7uzcsu5MGiuqcKQfxDJJMNkSr89iSRjeCqMZtUSAaC0PgmU5ibRAul6EZG4SOUQd5YglE8hERlk5vsWbCUXEC1eB2IJxlgggiaPI7V9bCaCWIJTGVwA4HWWbologuF2KBXrgLSDryQ+cWcJQtkUCqxnWyCCyspqNc3XlxkX0WpEWnSWSJ3LVvIdriYYHrteROQuuRIwM3whNcW3RItREITCIvL6BVvFBYYmEtm1Ilrab3NNWkS8av+sUpjRWyIOcWdVkqlwHAm1t5lYIoJQOnlFZKlUiC9Fmmu01if5LJG0O6u2DHdWIKoIhpbiC4g7q1JoQfV6lw0TIiKCUDLFNGAUsqhz2WC3mDA0lS0iYThtZtRUpcfoep02hGNJhGPF12dqVofHnrZEpPVJZdAEfnWjq+TPSRAEEZGyICI01zhymjAO+kNornGAKB1OKqdqPZWd5bCgymqGicQSqRRaUH11gxuApPkKQqmIiJRJc01VTuuTAV8oI6gOIF21XkLQdiYch4mAKqsZRDRvM0W+/WwPPvHIgTm/znJCE42uRpfyWILrglASIiJl0uzNbH0SjiVwdHAaG5qrM/bzqpZIKUFbbaqhZtF4HNZ5sUR+cWAAvz06MufXWU5MqDGRrgZFRCS4LgilISJSJi01VRiejqQye17pnUQ0kcTVq+sy9ku3gy/+Dnc6HIfHkY6ruO1zbwcfTyRxbGgaE8Foas2C4s4ymwir6pQa2qVYcOgPxfDmrz2LY0PFzIIThIVFRKRMVtY4kEgyRqYVl9ZLPRMwEbC9M0tEyoiJBFRLRMNln3s7+NOjAUTiSTCL31/PZDAKb5UVda7yOy5XmjNjARwemEL3WeNWO4KwmIiIlEl6wqEqImfGsbGlBtU6CwJQKtaB0t1ZLrs59djtsGJ6jiJyeCDdeHl8ZuldKBcLXzAGr9Na1ue0UGjCNjodmWVPQcjFH4rhZ/v6MZSnaexcEREpk+aadMFhOJbAK70+XN1Vl7Of3WKG02YuzZ0VicOtEyOP3YKZOY5uPTyQdoWMzcjFSGMyGEWt01bW57RQ+NU1jcrnJpRBz+gM/uqHB3BksDITPEREyqRFExFfGAf6fIjGk9i5ut5w31qnrbQU33AsVWQIqDGRebBEqtWaExGRNBOBaCr5QWmWufQsEc06GpmSz00onWH1e6Pv8zefVFREiGgXER0nolNE9KkC+91BRExE29XHnUQUIqL96s83KrnOcqiussBpM2PAH8JLZyZABFxlYIkAahPGEi5OM1kxkbnOWWdmHBmYwmvWNgAQd5YeXzCGOjX5odZVeneBhUDr7SWWyMXJlx47judOjpV9vBa3XVFdGREpZsZ6WRCRGcD9AG4C0A9gDxHtZuYjWft5AHwEwEtZL3GambdWan1zRSs4HPKHcXxoGpetrM6oVNejWCKl1YloleqAYokEogkkkgyzqfS+mP2TIUyF47h2TQMeOzyM8YBcjDQ0dxZQ+ue0UGgFkWMSE7noYGZ84+nTGA9EUjeBpTI8FYbZRKiv0DyiSloiOwCcYuYeZo4CeBjAbQb7fR7AFwFUJupTQVq8VTg3HsQrvZN5XVmAYokUG7BNJhmBaCLDEvGogqL11CoVLai+qbUG9S6bWCIqoWgCkXgy5c4qp1nmQuDTBdaZJT37YiIQTSCe5NSNRDkM+SNo8tgzZh/NJ5UUkVYAfbrH/eq2FER0BYB2Zv6lwfFdRLSPiJ4motcanYCI7iaibiLqHh0dnbeFF0tzjQNHBqcQjuXWh+gp1A6emRHQxTs0ofBkWSJA+f2zDg9MwWwirF/pQb3bjjEREQDprCetq4DSLHMJWiKqOyuaSMIfWnrrEyrHfEzdHJkOV8yVBSxiYJ2ITAC+DOBjBk8PAljFzNugzHh/iIiqs3di5geYeTszb29sbKzsgg1YqWv5vqMzv4h4nTZMhWOGRX4/23ceV//jb1MXBy2A7sqKieifK5XDA1NY0+iCw2pGg9tWEXfWRCCK2+9/Hn0TwXl/7UqhVavrLZF8n9Niohc2SfO9uEgPTCv/5mF4KowV1fb5WlIOlRSR8wDadY/b1G0aHgCbADxFRGcB7ASwm4i2M3OEmccBgJn3AjgNYF0F11oW2gTD9Ss9Beef1zqtYIbhXeSesxOYicRxRE3B1c9X13DPcUTukYEpbGypAYCKubOODU5hf58PB/srk0ZYCbQ/TK3QsNDntJj4g9HURUBE5OJCs0Dm8p0cnopcsJbIHgBriaiLiGwA7gSwW3uSmf3M3MDMnczcCeBFALcyczcRNaqBeRDRagBrAfRUcK1loc1SLxQPAQpXrR8dnAYAHBlURGRa18FXwzMHS2R8JoKhqTA2tiiGnOLOmv8LkfYln55jPctc+eoTJ/CFR48WtW+uO2tpVq37QjGsbfIAkAytiw3tu1iuJRKOJeAPxS5MEWHmOIB7ATwG4CiAHzHzYSK6j4huneXw6wEcJKL9AB4BcM9SHJK1tskNm9mE11/WVHC/fNXQySTjxLAqIlmWSGadiDXjuVLQigw3pETEhmA0gWCZQfp8pEVkcVvWP3NiFL85MlzUvtrnkXZnLb2q9USS4Q/FcEmT0qpeakUuLjTxCMUSZc26GVZnHjV5KufOqliKLwAw86MAHs3a9pk8+96o+/0nAH5SybXNBy3eKhz47M2ospkL7pe6w81qM94/GUIwqnwxNEtkxsASScdESr8b0URkY7PizmpwKV+m8ZkonHXGH/9MJI5AJF7S3YsmIlOLbIlMh+MY8ofBzBlzXYyYUD8Pb5YlMrGE2sFPh2NgBtrrnLBbTGKJXGToLRB/KAaHtfC1Jhut0HBlzQVoiVwszCYggHL3DwDD05lZzFpX1p2r63BqZBrReDItIvMUEzk84Eertwo16oVSW8t4geLHL//mBN7xzRdKOs9SsUSmw3GEYglMFbGOyWAUHrsFVrPyZ7AUmzBqF5FapxWNHrvERC4y9N/FclxamiVyQbqzhDSt3irUuWx45ZwvY/uxIcWV9dZtrYglGCdHpnXurMxW8EB5MRElqJ5ObKt3a5ZI/otR70QQfRPBkrKUUpbIIgeltZjM8NTsZUe+YDQjIWIpurMmUy43K5pERMomEIlfkKOPfRkiUvr3MiUiFWp5AoiILAhEhKs6a7HnbGZY5/jQNFbVOVPt448OTutSfNMWjtlEcNrMJcdEovEkzowHsF43KEurWi2UoTURiCDJKCkVOO3OWjxLJKEWagLImTppxGQwlgqqA4pYW0y0YLUio9MR7PrqMzgzFsi7j1Yj4nXa0Oixp1pYCMXDzPjDb7yAv/npocVeSslMBmOwqZZyOd/L4akw7BYTqqsqF7kQEVkgruqsQ+9EMKMd87GhKVy60oPOeheqrGYcGZjCTCQOh9UEiznzoymnCePwVBjMQJtuZG+DaomMFRAIrX6ilLvepZCdpRfZ4SJExBdMN18EFLFfyKr1Q+d9ODY0jX29+eeEaB18vVXiziqX7nOTODI4ldHJ+kLBF4xiVb0yMM0fKscSUdJ7Z4sPzgURkQVih9qc8WXVGgnHEjgzFsD6lR6lmrzZgyODfkyH46lsLD1uh6XkmSLa3bg+qFZlM8NlMxe0RMbLEJGpJWCJ6IP6Q0W4syaC0QxLBFCr1hcosH5enUVT6H2e1GWQNbodmAzGEI0nF2R9y4UfvNwLQHHTXmhtY3yhGDrrtdHN5VkiKysYDwFERBaMDc3VcNnM2HNGEZFTIzNIMrB+ZXXq+SMDU5gOxzJanmh4yhiRO+hXZsBrA7Q06t32vDGRaDyZCo5faJaIPqhfjIj4ArEMSwQovW3/XDg/qXw+hd5n7cJRo1oiQGluxosdfzCGXx4chMdhQR+YC6oAACAASURBVCiWuOAsuclAFK1eB2xmU8q1WQoj0xE0VbBaHRARWTAsZhOu6EjHRbSg+qUrlSKyDS3VmArHcWJ4OiMzS8PtKN2dNZSyRKoytte7bXn7Z+kvoKWkky6FwLpewGab4hZLJDEdiacysjSUZpkL838Y8KkiUuB99odiqHZYYDZRKtdfakWK5+f7zyMST+KeG9YAAM5dQG154okkpsJxeJ021JTQxFWDmTHkr2zfLEBEZEHZ0VmH48PT8AWjOD40BZvFhE7V33mZGvw+MTxjLCJlWSJheOyWnNerd+WvWtdvL/aujZlTbqyZSHzRXAaaJdLgts0qIvrUWT0LaYloIlJIFCZ1cRvNErnQ7qYXC2bGD17uxebWGrxpczMA4Nz4hSMi2o1ZrdMKb1XpNzfTESXdvZJ9swARkQVlR1cdmIHus5M4NjSNdSvcqQD6+pUeaLEvt4E7y223lmyJDPpDhkVGShNG4wvlhG57sRermUgciSSj0WNHkpHKkFpoptVizEua3LOm+E5mVatr1LoUEVkIISzGEvHpMshSIiIFh0VxoN+PY0PTuHNHO1q9VTAR0DuePxNuqaFlY9W6bKh12koWkZEFqBEBREQWlC3tXtjMJuw5O4FjQ9O4dEU69dZps6CrQQmgGVkiHoel5HjDkD+c6u+lp95tw0QgiqRBHYgmInUuW9Eiot0xtdUq51osl5Zmiaxb4cF4IIpIPL+YaS22a3NiIlbEElxxIYwlkqm4TcGYSCiGGnWNWmadWCLF8YOXeuG0mXHrlhbYLCZl/s8F5M7St+WpcVpLjolo1eoiIssIh9WMy9tq8JsjwxidjmC9Gg/R2KC6tPK6s0p0FQ34w2g2+ALVu+ypnkzZaFlbl67wFH3HmxYRxTW3WFXr2nnXFtFnKn2Xl+vOAjCnWevMjL3nJgp+VsNTYSRZKUT1h2J5Bc8XjMKrTsy0WUyodVpFRIpgJhLHLw4O4C2Xt8DjUN6/jnrnBeXO8unSuxV3VmnfyYWoVgdERBacq7rqUsVl65uzREStLDd0ZzksSLLSiK0YovEkxmYihu6sdOuT3IvRRCAKEwFrV7hLtkTaVUukFIvJF4yWPSclm6lwDDaLCavUlMhCGVq+oLElkq5aL9+a2nN2Enf8+wt47lT+udgDanrv1nYvAORNdPBlFUQuZMFhMsn46x8fwN5zS6736aw8e2IUwWgCb70iPQdvVZ0L5y4od1b6O1pOwsfQAjRfBEREFhytXgRIZ2ZpFLJEtLTfl84U9wc9Mq0UGman9wJAo1ZwaHDhGg9EUeeyYUW1A9Ph4lpFTGVZIqU0Yfzj77yED36vu+j9CzEdjqPaYUnlxRcKrqcsEYOYiPJ8+ZbIWfVC1X02fxGhFg/RRMRIsBNJxlQ47c4CsKAFh4NTYTyytx//c3BwQc43nzx9YhQeuwVXdtSmtnXUOzEZjBX9/fz1q0PY9dVnMiaPLiQpS8RlhddpK7mT78hUBB67JWPAXSUQEVlgruyoBZESc9Au5hqbWmtgM5vQbGA93HTZCqxudOH9392D+393yjCeoWcwT3ovoO+flXuhnAhEMtZWzAUrZYnUaZZIcX90YzMRvHp+Ci/0jOOF0+NFHVOI6XAcHoc1JSKFguuTwSjsFlNOA03trn8uIjKoWhkH+n159zmvisjlbUp35RGDtU6FlA6+mjsLUG4AFiqwfla1mM8WaMuyFGFmPH1iFK9Z25BqrgkAHXXKTU5vES6tQCSOz+5+FceGpvH7efhulsNkMAqLieCxW1IWcinDqYanwhWvEQFERBacaocVW9q8uLytJqcVQYPbjif/+gbcuqUl57imagd23/savGlzM/75seP4wPe7C95RaSJiJEiaO8sozXdCtURKyQTKjokUG1h/qUexquwWE/7ltyeKOqYQWqFmdZUFDqupsCUSiOZYIUA6W2su7iytyPNAny9vXOS8L4RapxUdquvN6H3WAqn6uI1miSxE9pjmdi3U22spcnJkBoP+MG5YlzkyW2sfUkxc5BtPn8bwVAQ2swlPnxipyDpnYzIYg9dpVdrxVJX+vRyeCle0BbyGiMgi8MB7r8T/+8Mths+11Tpz+mZpuO0W/Ou7tuG+2zbiqeMj+PazZ/KeY0i9kBmJSK3TBiLjTr7jM1HUu+x5axICkThiicy2G/5QDGYTpc5VbOuTF3vG4bKZ8bGb1+HFngm81GN8x8fMOD40PWssQLFELCAiNNdUYbCAJTIRiBqONPZWWUFUXBfgfAyo4jUZjKE3TzbQgC+EFm8V6t3KZ2Fk8aWyc6rS62zyOBCOJUtugVMOmgXSNxnK+cznm0g8gW8/2zMvw9KeOq5c9K/PEhFNsM9NFBbFvokgvvlMD27b2oLr1zXi6ROji1L7pO/tVk6H6eGpSEW792qIiCwCTR5HyqVUKkSE917Tic1tXrxwunDg1m23pDJT9JhNhDqnDWMGGUjj2ZZI1sXtLf/2HL7yeKbVoFVVO6xm2Mymot1ZL/aMY3tnHd57TSca3HZ87cmTqeeYGS/1jONzuw/jNV/8HW756jP4y4f3F3y96XAs1UJ/RbW9YBPGAX8YLQYCazGbsL2jFr8+PFT2hWPQF0KHete7v8/YpTXgC6HVWwWr2YQ6p3E6darlSVZgHViYNF8ttpNIMvoqnBr77Ikx/P0vj+I/nj8759d6+sQo1q1woyUrvd1tt6DeZZvVnfVPvzoGEwGf3LUeN6xrQN9ECGcXIavLF4ylXJkpESnSyk8mGSPTYTRVODMLEBG5YNm5ug77+3wI5alnGPIXNmXr3bYcSySWSMIfiqHOZUO9K/cOeSIQRc9oINWyRcMfiqNG/bJ7HJaiApdjMxGcHJnBztX1cFjNuOeG1Xj+1Di6z07gld5JvPOBF/HOB17ED17uxfqVHly7ph7d5yYL1n5olggArKx2FMzO0iwBI95+ZRt6RgPYl0cAZkNzpVRZzYYiwsw4P5k+f75guS+Um0G2kCJyZiyQqnYuxaW199wEdh8YKOlcJ0dmAAD/8fzZOc39CETi2HNmEjdeajyyetUsab4v9Yzjl4cGcc8Na9DircIN65TXefr4wru0JjMsEc2dld8S+bcnT+LRQ4OpY2MJrni1OlBhESGiXUR0nIhOEdGnCux3BxExEW3Xbfu0etxxIrqlkuu8ELlmdT1iCcbec8YZQINTYUNXlka9y54TWNeCyfVuGyxmE+pdtgxfvTYPXsss0vCHYikRqa6yFmWJaPGQnauVbLV3X70K9S4bPvj9brzt679Hz2gAn79tI/Z95iZ8531X4b3XdCIaT+LV8/nbeWuBdUBJKBiZihgmIAQicfhDsbwi8qbNzXBYTfjJ3v5Z/x/ZTIVjmInE0V7rxObWGkMRmQrFEYgm0KoXEaOYiK5OQGM2EekZnSlpmFg+FOsjhNepF+NSROQLjx7DZ/771ZLOd2pkBhYTYWwmgp+8Uvr7rvHC6XFEE8mceIhGR50zr4sRAP7td6fQXOPAh65Xem2tqneis96JZ07mt/qz+XF337wkI+jTu7XvQL6YyEwkjq88cRIfeXgfXj4zsWCFhkAFRYSIzADuB/BGABsAvIuINhjs5wHwEQAv6bZtAHAngI0AdgH4uvp6gsr2zjqYTYQX88QRhvyhwiJi0PpEX60OKIF+/cVKE5HsgU/+UAzVekukCJNbi4dsalWyk5w2C/7ypnWIJxgfvWkdnv74jXjPNZ1w2izq/1dJ1ew+a5zinEgyZiJ6S8SOaCKJCYM7t3zdjTU8Dit2bVyJXxwYKPmuWMvMavY6sKW9BocHpnJat2uZWSlLxG03LIycDMZAhNR7C6Rz/kcMRKRvIog3fPlp/KJEK8CIAV8I0UQSW9u98Dqt6CnyojgVjmFfnw++YCyjhc5snBqZxo6uOmxpq8G3nukpWwifPjGKKqs59X3JZlW9CwP+UF6L9sTwNK5d05CRtXfDuka8cHq8oBWsMTwVxscfOYh3fPOFOQvJpG7yptNmhtVMed1Zr5ybRCLJcFjMuOc/92KvOqPmghYRADsAnGLmHmaOAngYwG0G+30ewBcB6K9MtwF4mJkjzHwGwCn19QQVt92Cy9tq8IKBiMQSSYxMRwzTezUa3LlNGCdUy6TepVyost0sx1U3lj8UywiATuktEYe1qGJDLR6iT8F8z84OHPzczfjw69fm5LY3uO1Y3eDCnjy1F1rBYkpEavLXimhzPPJZIgBwx5VtmArH8cTR4Vn/L3o0K625pgpb22sRjSdxbGjKcJ/W2kxLJDsG4w9GUe2wwmxKZ/HVVFlhNZOhJbLn7ASSDBwdyrXWfMEofrint+gaCc3y6GxwoavBVfQF8YXT4ykBOD06U9QxzIzTowGsbXLjnhvW4Ox4EL9+daioY7Nf56kTI7h2TT3sFuN7zo46J5iB/slQznPhWALDU5FUPEvj+nWNCMUSBet+NA6olqcvFMMfffsl9E+WF0sJRROIxJOpWEh6YJrx5/fSmXGYTYQf3L0T8UQS9/3iMABc8O6sVgB9usf96rYURHQFgHZm/mWpx6rH301E3UTUPTo6Oj+rvoDYuboeB/p8ORktI9MRpdCwgCXS4LZhOhzPuLvSLBMtBThbRDRLBMi0RvTuLKXHV2F3lj4ekk2hCWxXdtRi77kJQxeVJlzVDi2wnr9WZCDLEjDi2jUNaK5x4JESXVoDOitnS7tiZR3IcmkNZFlCjR47ovFkTlabLxRLXUQ0iEipFTEQkVfUu89zY7kXrof39OGTPzmE6//v7/DAM6dntbC0oHqXKiLFurOePTkKTfN6ihSRoakwZiJxXNLkxs0bV6KrwYVvPH265MSGM2MB9E2EcOOlxq4sAOhsyF8rorm5skVk5+p6NdV39mvMwX6/cjH/4NWYDsfw7m+9NGtHaSOM4mGFWp+8fGYCm1trsKm1Bvf/0RXQ/kQaK1ytDixiYJ2ITAC+DOBj5b4GMz/AzNuZeXtjY/4vznJl5+p6xJO5cREtvbdwYF35culdDlqgXXNn6WsStDTbS9S+VJrbhplzRGS2u93seEixXNVZh8lgDD1juRcnTbg0S6RZtcKMguuDvhBMBKwo8AdmNhHeuq0Vz5wYNSwEzMegL6zO/nCg1VuFBrc9J0B/fjIEm9mEBp3FB+TGOSZ12Tl6VtU7cXw419p45ZxynrMGrT1Ojcyg1mnF5W1e/OOjx3DDP/8O//LEybxtQM6MBeC0mdHksaOr3oVBf7io9NtnT47hhnWNsFlM6BktTnhODiuf55omN8wmwt3Xr8ah8/6Si/x+c0SxGrVguBGr6tQ0X4P/txZwX1WXKSIuuwVXddXimSJE5EC/D+tWeHBlRx2+//6rMRGI4pavPoPP7T6MwwP+ov8v2nRN/eefr/VJOJbAgT4/rla7Ybx2bSO+8NbNeNu21rwW2XxSSRE5D6Bd97hN3abhAbAJwFNEdBbATgC71eD6bMcKALZ31MJiopxqb60vU/Ms7iwg3ekTUASFKH330+hW4gpToTiGpyKYCsdxoxqw1O6mA9EEEknOcmcVvti8dCYzHlIsmp/byKWVFhGr+v+zwUTGs9bP+5RBPfnqcTTuuLINSVYGGxXLgD+EFR47zCYCEWFre02OJXLeF0KL1wGTesueT0T8WTPgNa5d04DDA1MZNwCBSBzHhqZgNhHOjeeOgT09OoP1K6vx/f+1Aw/fvROrG9z4yhMncMM/P4W3fv35nAvk2bEAOupdICJ0NbrUbYVdM+fGAzg3HsSNlzahs96J00WKyCk1M2ttk9IG6G1XtKLJY8e/P3W6qOMB5b37+u9O4ZrV9amiQiMa3DY4bWbDbr5pS8SV89z1axtxbGi6oFXBzDh03o8taheCre1e/PBDO/HatQ146OVevPlrz+FtX3++KJeiz2BUQU2VzTAmsq/Xh2giiat1N2XvuKodX37n1lnPMx9UUkT2AFhLRF1EZIMSKN+tPcnMfmZuYOZOZu4E8CKAW5m5W93vTiKyE1EXgLUAXq7gWi9IXGpcJDu4rn3Rm/MEjoF0p9tjg+k72vGA0jHWnH1xmwnjuOrK0gq4NEtEq1ZPWyJWBKOJgsVpRvGQYuhqcKHeZUtNh9SjubM0S8RiNqHRY89JAgAKp/fqWdPoxrZVXvz3/uID1YO+zPb7W9u9OD0ayGhXkX3+pjzdAYzcWQDwmrUNYAae1zV4PNjvR5KBG1X/vT7wzszoGQ1gtSoGO1fX4wd378Tzn/oDfHLXeoxMRfDxRw5kuAnPjgfRpbp+tBEFRhaOHi2D6bVrG7C6wV20O+vU6AxqqqxoUN2odosZ739NF547NYaDBVrH6PnCo0cRiiXw+ds3FtyPiLCqzmnszhoPwG235AwqA4AbVBfZsyfzWyO9E0H4gjFc3uZNbdvYUoN/e/cVePlvXo9P7lqPV3p9RcV7jLpM1zqt8Bu4s146Mw4i4MqO0iz7+aJiIsLMcQD3AngMwFEAP2Lmw0R0HxHdOsuxhwH8CMARAL8G8OfMvDiTjpY416ypx8F+f0aTuEF/GC6bGZ4CjddW1TnhtltweCAtIlrLE41GXSbQSVVENrXWoMFtT2U4+YOZIlJdpZwz3xTG8ZkITgwbx0Nmg4iwvbPWMMCZ7c4ClDRfI3fWgL84EQGUaZQnh4tPmx3MyorbojZYPNSfdmUM+MIZ5290K/tnu80mA1FDd9blrTWodljwnC7tVIuH3L5NCR3qA+ETgSj8oRjWNLozXqfVW4U/vXENPnrTOgxPRfCq6m6JJ5LomwiiU70j1/6dLS7y7IlRtHqr0NXgwupGF3ongkVVup8amcElTe6MeNi7r16FaoelKGvk96fH8NN953H39atxSZNn1v076p2GgnhuIohVdU7DuNylKzyodljwSm9+UTugfsZaPzQ9XqcN99ywGq3eKjxWlIgYxESc1pS46Hn5zAQ2NFen/gYXmorGRJj5UWZex8xrmPkf1G2fYebdBvveqFoh2uN/UI+7lJl/Vcl1XshocZFuXVxEm2hYKEhtMhE2NFdn+GnHA9FUZhagu0OejuD40DQaPXbUuWxo8TpSrT2MLBEgfxPGfeofYb4UzNm4qrMOvRPBnAtu2hJJ/yGtrLbnBNaTScagP5w3vTebzgYXoolkTm2MEczaa6cFQrsr1eJW0XgSw9OZ+1RXWWAzmzIsEaWDb9zQnWUxm3DtmgY8ezLdjmNf7yRWN7pSXYH1F0nNraRZItn8wfommAh4XI0p9E+GEE8yOlULxGW3YEW1vWCMI5ZI4oXT47h+XSOICKsb3YgnuWBNhsapkZmUZazhcVjx3ms68evDQyl3lxHReBJ/9/NX0V5XhXtft3bWcwGKu6pvMpSToNE7HswJqmsQEba0e3Nck3oO9vlgt5hyunPrX+OWjSvx7MmxWccfpN1Z+phIbiffaDyJV3onM7qDLzRSsX6Bc2VHLazmzHqR7AtZPja2VuPo4HTqLjvHElHvkEenIzgxrIzzBZRq8EH1oqqJiL5OBMjfDv5gvw9mE2FTS2nxEI3tncofS3dWMsGUkSVS7cjxYY8HoojGk2gpEC/SU6wrB1Dev0g8mWGJ1FRZsaOrDg88cxrHhqYwPKW06G/TfT5ElJMJp9XaGLmzAMWlNeAPo2csAGbGK70+XLGqFs01DljNlNGmQ3MrZVsiGrUuG7Z31qVE5IwuM0v/PpwxSGjQONDnw3QkjuvXNqjncqnnLvy+TQSimAhEUwkbev7kuk7YLSY88Ex+a+TfnzqN06MB3HfrppyOzPm4pNGNaDyZ+n8CanHlZLBgPGVLmxfHh6fzdok42O/Hhpbqgm7aN25eiWgiiSePFa6A9wVjcNrMGYFx7bugr8M6dN6HcCyJq7tKt+znCxGRCxynzYItbV786tBg6m58yB9OtUMvxMaWGoRiiZSbYiIQTaX3Auk75JFpxQW1boVyh9XirUrFGqayLBEtxTafiOzv92PdCk/Rf/C5a66Gw2rKiYtMh+OwmU1wWNOvu6LGgalwPCOrqJj0Xj3ahbSYFNd05+TM1/7andvgslvwge9149B5v+H5G7JEZNLgTlTPa9WL9XMnx3BuPIiJQBTbVnlhMZvQXufMyD7qGQukxsPm4+YNK3BsaBp9E8GUK6yzXi8i7oL9o545OQYTKUF/AFitCtZscRHNylhjICL1bjveub0dP9t3PuU+1ZiJxPGJRw7gK0+cwJsvb8br1ufPyMrmcjX1Wh9vGfSHEEswOuqMrTVAcU0mkowjg7lZVokk49UBP7bo4iFGXLGqFg1u+6wuLaPMvFQnX52IvKhmOoolIsyJv3zDOvRNhnDvQ/sQjiUwMl245YnGRnWS4uEBPxJJxmQwinqdJaLdIe/rnUQolsClqog01zgwE4ljKhxLu7OcWZZIKNdcZ2Yc7PelslfKwWo2YVt7blxEawOvp11tTa+/G06LSHHurCaPHU6buSgRyffaK2sc+NZ7t2N0OoJPPHLQcJ+mLBHxpSyRXHcWoLhkVtU58ezJMezrU96LK1YpLsLOehfOjGVaIqsbXBlFi9m8/rIVAIAnjg7j7JgSYG7Q3VB0NTgxEYjmrVN49uQotrR7U98DLVA+W8FhOjPL2Er64PWrwQx8+TcncHjAj7NjAfz+9Bje9C/P4pG9/fizG9fgK+8oLQvpkkY3qqxmHOhLi4HmdstO79WjfW/39+WKyKmRGQSjCcN4iB6ziXDzxhX43fGRgrU6PoPMPO2GQj+6+eUzE1i3wp3hQVhoRESWAa9Z24C/v30Tnj4xig//YB+SjIwMoXxc0uSGzWLCkYEpTAajYEbOl7HBY0/FMdapvl7ttYf8YfhDMZgIcKvtSapTMZFcS0TLXtGCzeWyvbMWhwf8GRaGvvmihhYf2NebFhwtllOsO4uI0FFfXMV2PksEUO5i//kPt6R84dlWQaMns4OA36BvVjavWduAF3vG8fKZCbjtlpSlqMwSD6TiJad1mVn56Gpw4ZImNx4/Mowz40F0NmQGmLsalIu8kZge7Pdhf58Pf5DV9FDJ0Cr8vp0cmUaV1Zz382irdeJtV7Tix3v78eavPYcbv/QU3v2tl5Bkxg8/dA0+sWs9bJbSLmMWswmbWqszLBEtWytfTARQZvq01DgM4yLaALLLZ7FEAGDXxpUIRhN4tkA/LqXlSeZnr1n72g1GPJFE99mJRXVlAUBl5yYKC8a7dqzC2fEAvvl0D4DChYYaVrMJl67wZNQc1GW1qG902xFXYyba3aJWCT/gC6X6Zmk1D1p2llFgXWtGONvd2mysX1mNJCsXtI1qbEWxRDL/6Npqq9DoseOVXh/ecw1Sa66ymvO6iYxY3eDCkcH8jR81BvxKEWF9nrvCW7e04PxkCC+fGc9wuwHK+zweiCKeSMJiNuncWfnvMF97SQMeeqkXP983gCs6vClLo7PehWA0gdGZCLxVNvROBPHmzc2zrv+mDSvwwDM9qHXacgpB9W69bavSSRHJJOPv/vswGtx2vO+6zoxjVje6UgWA+Tg1MoM1Ta7U98eIf3zrZty+tRVT4TgCkTgSzHjjppWGYw6KZXOrF//10jnEEklYzSacmwjCopuJk4/L27yGEysP9vvgsVuwuqGwWANKMky1w4JfvzqEmzasMNzHF4zl3AhqfbS0G4x9fT4EoolFdWUBYoksKz55y3q8afNKAGlXzmxsbFEytMZTfbMyL1pamm+rtyr1R6t9uQdVS0SfWqjNhzeKiRzs98NhNaXumMtFu1vU5/obWSJEhCtWeVPpr4BWo1E4cy2bzgZnUemqgz6l/X6hC+Kf3rgG//EnuW3gGj12MKdbz/hSM+DzXyivXdMAEwGhWCLlylLWmy4O7J0IIpHkWS0RAHjDZSuQSDLGZiIZQXVAcfOYKHdU7o+6+3Cgz4e/edP6nIv66kZXQRcYAJwemcEleQL+GhazCdde0oBdm1bijivb8I7t7XMSEADY0l6DSDyZqpbvHQ+irbZq1gLULe1enBsPZriUAOW7vam1puBnr2GzmPCGy1bgiaPDeb9TvlAs57PXrFLtBuM7z55BTZW1pHhQJRARWUaYTISvvHMrHr57p2G2ixEbW6oxGYylUn2z3VmaiOjTFld47DCR0j4kW0QsZhNcNrOhJXKgz4dNLTUlFxlmo2XQnJ1FRAAlTnBuPJhyFRVbaKins96FRJINm/bpya4RKYXsqvVD5/3KfO0CF8satY0JAGxblXajdKben8CsmVl6trV7U3GQzqyqbZvFhLZaZ0Y3X18wii/++hh2dNbh9q05re1S58xXuT4TiWPAH8baOd5UlIP2vmkurXMTAawyqFTPRuuHdvB8Oi4SiSdwdHAqFbAvhls2rYQ/FDPswp1MMnzB3PHN+k6+p0Zm8NiRIdx1TUfqxm2xEBFZZtgt5pIK+Tao7iDNP5vPEtFbDxazCU0epVYkW0QAJcc/ux18PJHEqwP+onzGs1HtsKLOZUOvbsypkTsLAK7oUO7QtbiOMtGwNBFJpfnOEhfJLiIsBb2I/PrVIfxs33m8/zVdBYPhAPC6S5tgNRO2tactkVZvFSwmwtmxwKw1InpMJsLr1yvulU4Dt8zaJjd+e3QEf/vzQzjU78eXfnMcU+E4/s9tGw0tu9UpETEOrp8eKV7g5pvOeieqHRYc6PeDmXFuPIiOAkF1jc2tNSDKbKp5bHAasQTPmpml54Z1jah2WPA3PzuU8/5Mh+NIMnL+rohIaX0SjOGBZ07DbjHhrms7iz5npRARuci5rNkDIqV1AoCcueONbk1EMv/Qm70ODPpDmNLNEtEw6uR7YngG4VgydSc3VzrqnRm9nPJZIptba2AxEfaqUxFHpyOlWyJFpPkmkozhWQaBFUIr7DzY78enfnoQm1tr8LGbL531uA/dsBr/8xevzfjc0mm+QfSMzqDRYy/a/fPuq1fh6q467j1iXgAAD09JREFUXNacax185i0bsGvTSvy4ux9v+bfn8J8v9uI9OztwWXO14Wu111bBaqa8wXUtM6tYq3k+ISJc3ubFwX4f/KEYpsPxgplZGh6HFWsa3Rki8uO9fTARMlyKs+GwmvH991+NYCSBO/7999h7Lp2yblStrlHrtOLY0BR+tu887rxqVdljtucTEZGLHKdNCQaGY0lUOyw5rqYrOry4dk09XnNJQ8b2lpoqDPqMLZHqKiumI5mWiBaMLOVurRD6CXXJJGMmGje8UDqsZmxsqcYrvZOpwsNi03s16l02eByWgiIyNhNBPMllWyJaQ8x/ffIkovEk/uXOrUVlHTmsZsMKaa21R89YoKhgr8aWdi9++KFrUsPAMl/Tha+8cyte/t9vwOdv24i3X9mGv7ppXd7XsphNWFXnNKwV6Rmdwc/3n4fVTAUzoirJ5W01OD40jRNqXKRQoaGeLWpwnVnpoP1fL/Xifdd2FZXMomdruxc//bNrUeu04d3fegnffPo0fnFgIDXDJjs7C1DSfPf1+pBk4AOv7SrpfJVCsrMEbGypwenRQOpCpqfJ48BDH9yZs31ljQO/PTaMeIIN3FmWnNG7B/t9qKmyztsFo6Pehf8+MIBIXBnewwxUG1giALBtVS0e3tObEp1SL/REpAxmKlC1Xmr9STYOqxnVDgumwnH8460bU66gcumsd2HPmQlYLSa8qYjMrFKoqbLiPdd04j1F7Lu60Z2KoyhDo0bx3efP4ukTo7CaCffcsGbOMbJyubzNi3iSUw0Ri/1ubm2vwU9e6ce58SD+5qeH0FztwMduzi+mheiod+Enf3otPvj9bnzhV8cynjNKFa9RCw5v29KCtiKTZyqNiIiATa3V2H1goKSCpeYaB8IxJbMkxxJxWHPiB/v7/Li8raakrKhCdNQrE+r6JkKp6ncjdxagxEW++/uz+O1RpdVEOdZCZ70rI8srm0I1IsWydVUtGt12/OGVbWW/hkZHvROBaAKIJkqyROab1Y0uPHV8BP9zcABf/91pHBmcQpPHjr96wzq86+p2NHkqP741H5pr9ZeHlC7NxbizlOMUa/pjPz6A48PT+NZ7t+dM4iyFOpcNP/7QNRjwhxCKJhCIJmAxkaGbUEtN/9ANa8o+33wjIiKkai1KERH9hdjIEtFP6QtFEzgxPI03XDZ/X/xUmu9EILWWfH7/K9TMpUcPDQJAWXGLzgYXfnFQsXyMBv2kLJE5iMj3/uQqAIWnOxaLPjC+GIFr/bljCca9D+1DV4ML//z2y3Hb1taSCwQrwcpqBxo9dgxPRdDosRu68IxYv7IaNrMJe89NYtfGlXlrPUrBZKKiLIs/3tmBLe3evE0eFwMRESHV/kTfN2s2spsM6vGoc9aZGUSUaqsyX/EQID046OxYMCUe+SyRVm8Vmjx2jExH0OC25RT6FcPqBheYlXqC7JTUcCyB50+NwWkzp4oty2G+rDQgM0W3mMysSnHjpY148+XN2LVxJd60uXnWbLOFhIiwpa0GTxwdKdoKAZR05w0t1Tg1MoPP3Vp4fsl8s7Xdm+rEsFQQERHgddpw1zUduLGEoqVClkh1lQWxBCMST8JhNacr1ecpMwtQgt0umxm9E8HU3Ox8lohSdFiLXx8eKtvdpM/Q0ovIK72T+OsfHUDPWAAfef3aeRWCudBWWwWziWCm4u5wK0WTx4H7333Fop1/Nja3evHE0ZGi0nv1/MNbNyEcS5YcTF+OiIgIAID/c9umkvZvcNthMRHiSaPAerpltcNqxm8OD2Ntk3te/d+pnlbjAWwLe9Xz5v86X9Hhxa8PD5Ud+O6qz20J/9UnTuBrvz2J5poq/NcHrsZ1WRlsi4nVbEJbbRXsFtOSuvtfamg3NsVmZmlsLHOUwXJEREQoC7OJsKLagfO+kEFgXWt9Ekc8GcLLZyfwsQKpoOXSUe/E8aFpw1ki2VypFh2Wm4Jb47Si1mlNdcf96Sv9+OoTJ/HWba2477aNc27DUQnevWPVkog9LGWuWFWLNY0uXFPGpE1BQUREKJsWryIi2cWG+k6+Tx5Tct7fsqVl3s/fUe/CE0eHU3OnqwtcyDe21GB1owvb5zCHulMdzNQzOoO//fmr2NFVhy/94ZYle6e/lDJ4lio1VVb89mM3LvYyLmgqeptCRLuI6DgRnSKiTxk8fw8RHSKi/UT0HBFtULd3ElFI3b6fiL5RyXUK5dFcUwUi5Mxy9+gskd0HBrClrcawjcZc6ah3IpZgnBiegdVMsBe463ZYzXjyYzfizZeXXzPR1eDC6dEA/uIH+2CzmPAvd25dsgIiCAtFxUSEiMwA7gfwRgAbALxLEwkdDzHzZmbeCuD/Aviy7rnTzLxV/bmnUusUyufq1XW4qqMup3OpZpkc7PPh1fNTFbFCgHSa76vn/fA4rBUPanfVuzA6HcHhgSl86e1b5lQTIgjLhUpaIjsAnGLmHmaOAngYwG36HZhZP6TBBYAruB5hnvmjqzvwo3uuydmuWSI/eLkXRJVxZQHpNN+esUDBeMh8oVWR/8l1nXjDPNQGCMJyoJJ/ea0A+nSP+wFcnb0TEf05gI8CsAH4A91TXUS0D8AUgL9l5mcruFZhHtGCzAP+MHaursOKIua9l0NztQM2iwnReHJBROT1lzXhi3dsxu3bctueC8LFyqKnbjDz/cy8BsAnAfytunkQwCpm3gZFYB4iopweAER0NxF1E1H36Ojowi1aKIjLZobm4bp1S+UuuCYTob1WrVa3Vz47ymE1451XrTKsWBeEi5VKish5AO26x23qtnw8DOB2AGDmCDOPq7/vBXAaQE6OKDM/wMzbmXl7Y2PjvC1cmBtEyjAli4nwxk0rK3ourTJ7ISwRQRByqaSI7AGwloi6iMgG4E4Au/U7ENFa3cM3Azipbm9UA/MgotUA1gLoqeBahXlmRbUdN17alDOfZL7RisSWYp2GIFwMVOz2jZnjRHQvgMcAmAE8yMyHieg+AN3MvBvAvUT0BgAxAJMA7lIPvx7AfUQUA5AEcA8zT+SeRViqfOeuqxZkbKdYIoKwuFT0L4+ZHwXwaNa2z+h+/0ie434C4CeVXJtQWdpL7EVULpolkm+WiCAIlWXRA+uCMBfSloi4swRhMRARES5oOuqc+Mjr12JXhQP4giAYIz4A4YLGZKKCc74FQagsYokIgiAIZSMiIgiCIJSNiIggCIJQNiIigiAIQtmIiAiCIAhlIyIiCIIglI2IiCAIglA2IiKCIAhC2RDz8hgmSETTAI4v9jqWCA0AxhZ7EUsEeS/SyHuRRt6LNJcys6fcg5dTxfpxZt6+2ItYChBRt7wXCvJepJH3Io28F2mIqHsux4s7SxAEQSgbERFBEAShbJaTiDyw2AtYQsh7kUbeizTyXqSR9yLNnN6LZRNYFwRBEBae5WSJCIIgCAuMiIggCIJQNstCRIhoFxEdJ6JTRP9/e/cWKlUVx3H8++OodFTwFoh5QUMp7OIFCbsQYj1kSQZFJkYiRiBRFt2slwjqoYgyS4RSy0CsMCvpQRKVCioL07wGhVkqxxulXclLvx7WOjWcztHOeJyts/8fGGbWms3w35v/zH/22nuvrdlFx1NLkgZKWitpm6Stkmbl/t6SVkn6Jj/3KjrWWpHUIGmDpPdze4ikdTk/3pTUpegYa0FST0nLJH0tabuky8uaF5Luz9+PLZKWSjqnLHkhaZGk/ZK2VPS1mgdK5uZtsknS6JN9/llfRCQ1APOACcBwYIqk4cVGVVPHgAdsDwfGAnfn9Z8NrLY9DFid22UxC9he0X4aeN72UOAnYEYhUdXeC8BK2xcCI0jbpHR5Iak/cC8wxvbFQANwG+XJi9eA61r0tZUHE4Bh+XEXMP9kH37WFxHgMuBb2ztsHwHeACYVHFPN2G6y/WV+/Qvph6I/aRsszostBm4qJsLakjQAuAFYkNsCxgPL8iKl2BaSegBXAwsBbB+xfYiS5gXpwupGSZ2ArkATJckL2x8BP7bobisPJgGvO/kM6Cmp34k+vx6KSH9gV0V7d+4rHUmDgVHAOqCv7ab81l6gb0Fh1doc4GHgr9zuAxyyfSy3y5IfQ4ADwKt5aG+BpG6UMC9s7wGeBX4gFY/DwHrKmRfN2sqDdv+e1kMRCYCk7sDbwH22f658z+k87ro/l1vSRGC/7fVFx3IG6ASMBubbHgX8RouhqxLlRS/SP+whwHlAN/47vFNap5oH9VBE9gADK9oDcl9pSOpMKiBLbC/P3fuad0Pz8/6i4quhK4EbJe0kDWuOJx0X6JmHMaA8+bEb2G17XW4vIxWVMubFtcB3tg/YPgosJ+VKGfOiWVt50O7f03ooIl8Aw/KZFl1IB8xWFBxTzeQx/4XAdtvPVby1ApiWX08D3qt1bLVm+1HbA2wPJuXBGttTgbXALXmxsmyLvcAuSRfkrmuAbZQwL0jDWGMldc3fl+ZtUbq8qNBWHqwA7shnaY0FDlcMe7WqLq5Yl3Q9aSy8AVhk+6mCQ6oZSVcBHwOb+fc4wGOk4yJvAYOA74Fbbbc8uFa3JI0DHrQ9UdL5pD2T3sAG4HbbfxYZXy1IGkk6waALsAOYTvrjWLq8kPQEMJl0NuMG4E7SWH/d54WkpcA40vT3+4DHgXdpJQ9ykX2JNNz3OzDd9gln+a2LIhJCCKEY9TCcFUIIoSBRREIIIVQtikgIIYSqRREJIYRQtSgiIYQQqhZFJIR2kHRc0saKR4dNYChpcOVMqyGcDTqdfJEQQoU/bI8sOogQzhSxJxJCB5C0U9IzkjZL+lzS0Nw/WNKafG+G1ZIG5f6+kt6R9FV+XJE/qkHSK/neFx9IaixspUL4H6KIhNA+jS2GsyZXvHfY9iWkK37n5L4XgcW2LwWWAHNz/1zgQ9sjSHNabc39w4B5ti8CDgE3n+b1CeGUxBXrIbSDpF9td2+lfycw3vaOPCHmXtt9JB0E+tk+mvubbJ8r6QAwoHKajTyV/6p8oyAkPQJ0tv3k6V+zEKoTeyIhdBy38bo9KuduOk4ctwxnuCgiIXScyRXPn+bXn5BmFAaYSposE9ItSWfCP/eE71GrIEPoSPEvJ4T2aZS0saK90nbzab69JG0i7U1MyX33kO4u+BDpToPTc/8s4GVJM0h7HDNJd90L4awSx0RC6AD5mMgY2weLjiWEWorhrBBCCFWLPZEQQghViz2REEIIVYsiEkIIoWpRREIIIVQtikgIIYSqRREJIYRQtb8BwpC5a6tRQswAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k0LLIIuVwd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB0wFmqKDuIp",
        "colab_type": "code",
        "outputId": "b922f26c-b6eb-4524-fc45-b5d31f180739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "data_augmentation = True\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "te1=[]\n",
        "for t in history.history['val_accuracy']:\n",
        "        te1.append(1-t)\n",
        "\n",
        "plt.plot(te1, label = k)\n",
        "plt.xlim([0,epochs])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Error')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8204 - accuracy: 0.3371 - val_loss: 1.6325 - val_accuracy: 0.4203\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.5338 - accuracy: 0.4462 - val_loss: 1.3288 - val_accuracy: 0.5225\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 1.4092 - accuracy: 0.4969 - val_loss: 1.2707 - val_accuracy: 0.5534\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 1.3138 - accuracy: 0.5328 - val_loss: 1.1547 - val_accuracy: 0.5903\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 1.2373 - accuracy: 0.5589 - val_loss: 1.0961 - val_accuracy: 0.6141\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.1789 - accuracy: 0.5855 - val_loss: 1.0956 - val_accuracy: 0.6121\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.1333 - accuracy: 0.5999 - val_loss: 1.0080 - val_accuracy: 0.6510\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.0903 - accuracy: 0.6152 - val_loss: 0.9759 - val_accuracy: 0.6561\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0589 - accuracy: 0.6289 - val_loss: 0.9237 - val_accuracy: 0.6729\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0252 - accuracy: 0.6416 - val_loss: 0.9587 - val_accuracy: 0.6624\n",
            "Saved trained model at /content/saved_models/keras_cifar10_trained_model.h5 \n",
            "10000/10000 [==============================] - 1s 86us/step\n",
            "Test loss: 0.9586744961738587\n",
            "Test accuracy: 0.6624000072479248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaUSfeIxyVZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "te"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPRpHLH3GCFR",
        "colab_type": "code",
        "outputId": "4b9e0c92-50f9-4eac-e5ea-ab46225af6f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history= model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks)\n",
        "   \n",
        "\n",
        "for t in history.history['val_accuracy']:\n",
        "        te.append(1-t)\n",
        "\n",
        "plt.plot(te, label = k)\n",
        "plt.xlim([0,epochs])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Error')\n",
        "plt.legend(loc='upper right')\n",
        "  \n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('k Test loss:', scores[0])\n",
        "test_loss[k]=scores[0]\n",
        "print('k Test accuracy:', scores[1])\n",
        "test_error[k]=1-scores[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0489 - accuracy: 0.9829 - val_loss: 1.4799 - val_accuracy: 0.7563\n",
            "Epoch 2/100\n",
            "Learning rate:  0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0564 - accuracy: 0.9803 - val_loss: 1.6999 - val_accuracy: 0.7471\n",
            "Epoch 3/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0548 - accuracy: 0.9810 - val_loss: 1.5569 - val_accuracy: 0.7561\n",
            "Epoch 4/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 229ms/step - loss: 0.0536 - accuracy: 0.9811 - val_loss: 1.7073 - val_accuracy: 0.7431\n",
            "Epoch 5/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0526 - accuracy: 0.9817 - val_loss: 1.4797 - val_accuracy: 0.7533\n",
            "Epoch 6/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0504 - accuracy: 0.9832 - val_loss: 1.5149 - val_accuracy: 0.7562\n",
            "Epoch 7/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0644 - accuracy: 0.9772 - val_loss: 1.5238 - val_accuracy: 0.7586\n",
            "Epoch 8/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0541 - accuracy: 0.9815 - val_loss: 1.4891 - val_accuracy: 0.7559\n",
            "Epoch 9/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0485 - accuracy: 0.9822 - val_loss: 1.5006 - val_accuracy: 0.7549\n",
            "Epoch 10/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0510 - accuracy: 0.9829 - val_loss: 1.5001 - val_accuracy: 0.7592\n",
            "Epoch 11/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0424 - accuracy: 0.9855 - val_loss: 1.5550 - val_accuracy: 0.7548\n",
            "Epoch 12/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0542 - accuracy: 0.9810 - val_loss: 1.6221 - val_accuracy: 0.7488\n",
            "Epoch 13/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0477 - accuracy: 0.9839 - val_loss: 1.5877 - val_accuracy: 0.7536\n",
            "Epoch 14/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0554 - accuracy: 0.9816 - val_loss: 1.4936 - val_accuracy: 0.7547\n",
            "Epoch 15/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0471 - accuracy: 0.9831 - val_loss: 1.6364 - val_accuracy: 0.7604\n",
            "Epoch 16/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0528 - accuracy: 0.9810 - val_loss: 1.4319 - val_accuracy: 0.7603\n",
            "Epoch 17/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0468 - accuracy: 0.9835 - val_loss: 1.7247 - val_accuracy: 0.7436\n",
            "Epoch 18/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0491 - accuracy: 0.9826 - val_loss: 1.5583 - val_accuracy: 0.7520\n",
            "Epoch 19/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0475 - accuracy: 0.9839 - val_loss: 1.7111 - val_accuracy: 0.7458\n",
            "Epoch 20/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0450 - accuracy: 0.9844 - val_loss: 1.5614 - val_accuracy: 0.7516\n",
            "Epoch 21/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0461 - accuracy: 0.9837 - val_loss: 1.4638 - val_accuracy: 0.7679\n",
            "Epoch 22/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0444 - accuracy: 0.9840 - val_loss: 1.5914 - val_accuracy: 0.7570\n",
            "Epoch 23/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0477 - accuracy: 0.9837 - val_loss: 1.6932 - val_accuracy: 0.7349\n",
            "Epoch 24/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0398 - accuracy: 0.9864 - val_loss: 1.9659 - val_accuracy: 0.7287\n",
            "Epoch 25/100\n",
            "Learning rate:  0.001\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.0439 - accuracy: 0.9848 - val_loss: 1.5288 - val_accuracy: 0.7571\n",
            "Epoch 26/100\n",
            "Learning rate:  0.001\n",
            " 95/391 [======>.......................] - ETA: 1:04 - loss: 0.0470 - accuracy: 0.9828"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-540130fd1ebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSgoCFqdR65E",
        "colab_type": "code",
        "outputId": "5df0fb7b-75e9-48af-8e6a-7d0a0decf3ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "\n",
        "for x in arr:\n",
        "  re[x] = te\n",
        "re"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-36d816d47df4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrGJBWh3jmx-",
        "colab_type": "code",
        "outputId": "c249ecdc-6870-4053-e36e-f44ad08c18f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "te"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6087000072002411,\n",
              " 0.6371000111103058,\n",
              " 0.561599999666214,\n",
              " 0.453499972820282,\n",
              " 0.4556000232696533,\n",
              " 0.4383999705314636,\n",
              " 0.4301999807357788,\n",
              " 0.42570000886917114,\n",
              " 0.4697999954223633,\n",
              " 0.3928999900817871,\n",
              " 0.3708999752998352,\n",
              " 0.3733999729156494,\n",
              " 0.3587999939918518,\n",
              " 0.3758999705314636,\n",
              " 0.3758000135421753,\n",
              " 0.3514999747276306,\n",
              " 0.3496999740600586,\n",
              " 0.3174999952316284,\n",
              " 0.30809998512268066,\n",
              " 0.343500018119812,\n",
              " 0.3363000154495239,\n",
              " 0.2889999747276306,\n",
              " 0.2912999987602234,\n",
              " 0.3317999839782715,\n",
              " 0.3223000168800354,\n",
              " 0.3288000226020813,\n",
              " 0.2955999970436096,\n",
              " 0.2929999828338623,\n",
              " 0.27170002460479736,\n",
              " 0.32520002126693726,\n",
              " 0.28039997816085815,\n",
              " 0.29409998655319214,\n",
              " 0.2955999970436096,\n",
              " 0.2840999960899353,\n",
              " 0.27560001611709595,\n",
              " 0.292900025844574,\n",
              " 0.2831000089645386,\n",
              " 0.27160000801086426,\n",
              " 0.27069997787475586,\n",
              " 0.26260000467300415,\n",
              " 0.2784000039100647,\n",
              " 0.2906000018119812,\n",
              " 0.26319998502731323,\n",
              " 0.267300009727478,\n",
              " 0.2523000240325928,\n",
              " 0.2605000138282776,\n",
              " 0.2937999963760376,\n",
              " 0.26899999380111694,\n",
              " 0.29250001907348633,\n",
              " 0.26819998025894165,\n",
              " 0.30229997634887695,\n",
              " 0.26319998502731323,\n",
              " 0.26340001821517944,\n",
              " 0.26200002431869507,\n",
              " 0.2595999836921692,\n",
              " 0.265500009059906,\n",
              " 0.2702000141143799,\n",
              " 0.2742999792098999,\n",
              " 0.2506999969482422,\n",
              " 0.2573999762535095,\n",
              " 0.27719998359680176,\n",
              " 0.2515000104904175,\n",
              " 0.27090001106262207,\n",
              " 0.2797999978065491,\n",
              " 0.254800021648407,\n",
              " 0.25209999084472656,\n",
              " 0.2378000020980835,\n",
              " 0.258899986743927,\n",
              " 0.24970000982284546,\n",
              " 0.2519000172615051,\n",
              " 0.23500001430511475,\n",
              " 0.24169999361038208,\n",
              " 0.24449998140335083,\n",
              " 0.2289000153541565,\n",
              " 0.23909997940063477,\n",
              " 0.2311999797821045,\n",
              " 0.27240002155303955,\n",
              " 0.2516999840736389,\n",
              " 0.25690001249313354,\n",
              " 0.29509997367858887,\n",
              " 0.24330002069473267,\n",
              " 0.25110000371932983,\n",
              " 0.24229997396469116,\n",
              " 0.24279999732971191,\n",
              " 0.2444000244140625,\n",
              " 0.2713000178337097,\n",
              " 0.24449998140335083,\n",
              " 0.24010002613067627,\n",
              " 0.24199998378753662,\n",
              " 0.24769997596740723,\n",
              " 0.23339998722076416,\n",
              " 0.24070000648498535,\n",
              " 0.2710000276565552,\n",
              " 0.23420000076293945,\n",
              " 0.23110002279281616,\n",
              " 0.23879998922348022,\n",
              " 0.2437000274658203,\n",
              " 0.25269997119903564,\n",
              " 0.258400022983551,\n",
              " 0.2519000172615051]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCfH1C1n3ViP",
        "colab_type": "code",
        "outputId": "12f5c0bb-fa6c-4619-ca8e-701d6b243a1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "#test_l =sorted(re.items())\n",
        "#x, y=zip(*((int(x), k) for k in re for x in re[k]))\n",
        "index=[64]#,8,16,24,32,64]\n",
        "for val in index:\n",
        "  plt.plot(re[val],label='Resnetv2-20 Parameter Width=64')\n",
        "  \n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Test Error')\n",
        "  plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkS5EwinqWOX",
        "colab_type": "code",
        "outputId": "13f42834-4456-4e09-c409-45548497cdae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "re\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: [0.7765000015497208,\n",
              "  0.8394999951124191,\n",
              "  0.8355000019073486,\n",
              "  0.7064999938011169,\n",
              "  0.6861000061035156,\n",
              "  0.8131999969482422,\n",
              "  0.6491999924182892,\n",
              "  0.6739999949932098,\n",
              "  0.6301999986171722,\n",
              "  0.611299991607666,\n",
              "  0.6265000104904175,\n",
              "  0.7032000124454498,\n",
              "  0.6689999997615814,\n",
              "  0.6538000106811523,\n",
              "  0.615200012922287,\n",
              "  0.6297000050544739,\n",
              "  0.699999988079071,\n",
              "  0.7488000094890594,\n",
              "  0.7098000049591064,\n",
              "  0.6913999915122986,\n",
              "  0.7375999987125397,\n",
              "  0.7530999928712845,\n",
              "  0.5934999883174896,\n",
              "  0.8402000069618225,\n",
              "  0.6207999885082245,\n",
              "  0.6922000050544739,\n",
              "  0.7378000020980835,\n",
              "  0.6608999967575073,\n",
              "  0.5945999920368195,\n",
              "  0.7989999949932098,\n",
              "  0.8706000000238419,\n",
              "  0.5992000102996826,\n",
              "  0.7892999947071075,\n",
              "  0.5624000132083893,\n",
              "  0.845100000500679,\n",
              "  0.6213000118732452,\n",
              "  0.567799985408783,\n",
              "  0.7871000021696091,\n",
              "  0.5356999933719635,\n",
              "  0.6328999996185303,\n",
              "  0.6218999922275543,\n",
              "  0.6976999938488007,\n",
              "  0.7037999927997589,\n",
              "  0.5487000048160553,\n",
              "  0.546999990940094,\n",
              "  0.5496000051498413,\n",
              "  0.5555000007152557,\n",
              "  0.8130999952554703,\n",
              "  0.5255999863147736,\n",
              "  0.5347000062465668,\n",
              "  0.7629999965429306,\n",
              "  0.8285000026226044,\n",
              "  0.7675999999046326,\n",
              "  0.5266999900341034,\n",
              "  0.5584999918937683,\n",
              "  0.7851999998092651,\n",
              "  0.6602999866008759,\n",
              "  0.5218999981880188,\n",
              "  0.6468000113964081,\n",
              "  0.6998000144958496,\n",
              "  0.5203000009059906,\n",
              "  0.6874000132083893,\n",
              "  0.6717000007629395,\n",
              "  0.7459999918937683,\n",
              "  0.5087999999523163,\n",
              "  0.7543999999761581,\n",
              "  0.7369000017642975,\n",
              "  0.8501999974250793,\n",
              "  0.7364000082015991,\n",
              "  0.5692000091075897,\n",
              "  0.5703000128269196,\n",
              "  0.7894999980926514,\n",
              "  0.725600004196167,\n",
              "  0.634799987077713,\n",
              "  0.5329000055789948,\n",
              "  0.7756000012159348,\n",
              "  0.7044999897480011,\n",
              "  0.8023999929428101,\n",
              "  0.8303000032901764,\n",
              "  0.7095000147819519,\n",
              "  0.5751999914646149,\n",
              "  0.49330002069473267,\n",
              "  0.4941999912261963,\n",
              "  0.5058999955654144,\n",
              "  0.4952999949455261,\n",
              "  0.4934999942779541,\n",
              "  0.49320000410079956,\n",
              "  0.50450000166893,\n",
              "  0.49570000171661377,\n",
              "  0.49459999799728394,\n",
              "  0.48989999294281006,\n",
              "  0.5031999945640564,\n",
              "  0.5175999999046326,\n",
              "  0.4983999729156494,\n",
              "  0.5042999982833862,\n",
              "  0.5060000121593475,\n",
              "  0.49229997396469116,\n",
              "  0.49889999628067017,\n",
              "  0.491599977016449,\n",
              "  0.5044000148773193],\n",
              " 2: [0.7190999984741211,\n",
              "  0.6360999941825867,\n",
              "  0.6028000116348267,\n",
              "  0.5871999859809875,\n",
              "  0.5717999935150146,\n",
              "  0.5602999925613403,\n",
              "  0.5356999933719635,\n",
              "  0.5548000037670135,\n",
              "  0.5446000099182129,\n",
              "  0.5440999865531921,\n",
              "  0.5410999953746796,\n",
              "  0.5252000093460083,\n",
              "  0.5153000056743622,\n",
              "  0.5302000045776367,\n",
              "  0.48809999227523804,\n",
              "  0.4819999933242798,\n",
              "  0.5202000141143799,\n",
              "  0.4853000044822693,\n",
              "  0.4796000123023987,\n",
              "  0.4926000237464905,\n",
              "  0.4708999991416931,\n",
              "  0.47359997034072876,\n",
              "  0.4772999882698059,\n",
              "  0.5083000063896179,\n",
              "  0.4807000160217285,\n",
              "  0.4797999858856201,\n",
              "  0.48170000314712524,\n",
              "  0.49299997091293335,\n",
              "  0.49949997663497925,\n",
              "  0.4621000289916992,\n",
              "  0.49010002613067627,\n",
              "  0.45270001888275146,\n",
              "  0.4521999955177307,\n",
              "  0.4632999897003174,\n",
              "  0.43860000371932983,\n",
              "  0.4415000081062317,\n",
              "  0.4896000027656555,\n",
              "  0.44440001249313354,\n",
              "  0.4333999752998352,\n",
              "  0.4358000159263611,\n",
              "  0.45840001106262207,\n",
              "  0.4592999815940857,\n",
              "  0.42739999294281006,\n",
              "  0.4221000075340271,\n",
              "  0.45410001277923584,\n",
              "  0.4520000219345093,\n",
              "  0.430899977684021,\n",
              "  0.4405999779701233,\n",
              "  0.43730002641677856,\n",
              "  0.4625999927520752,\n",
              "  0.43790000677108765,\n",
              "  0.455299973487854,\n",
              "  0.41610002517700195,\n",
              "  0.4211999773979187,\n",
              "  0.4678000211715698,\n",
              "  0.4318000078201294,\n",
              "  0.4323999881744385,\n",
              "  0.4433000087738037,\n",
              "  0.4503999948501587,\n",
              "  0.406000018119812,\n",
              "  0.4067000150680542,\n",
              "  0.40859997272491455,\n",
              "  0.4049999713897705,\n",
              "  0.4214000105857849,\n",
              "  0.4092000126838684,\n",
              "  0.41750001907348633,\n",
              "  0.41130000352859497,\n",
              "  0.43870002031326294,\n",
              "  0.4050999879837036,\n",
              "  0.47450000047683716,\n",
              "  0.39480000734329224,\n",
              "  0.4416000247001648,\n",
              "  0.43650001287460327,\n",
              "  0.46399998664855957,\n",
              "  0.4294999837875366,\n",
              "  0.4211999773979187,\n",
              "  0.4196000099182129,\n",
              "  0.4092000126838684,\n",
              "  0.41589999198913574,\n",
              "  0.40060001611709595,\n",
              "  0.45329999923706055,\n",
              "  0.40689998865127563,\n",
              "  0.41130000352859497,\n",
              "  0.41860002279281616,\n",
              "  0.40630000829696655,\n",
              "  0.4128999710083008,\n",
              "  0.44279998540878296,\n",
              "  0.4405999779701233,\n",
              "  0.4000999927520752,\n",
              "  0.3895000219345093,\n",
              "  0.44520002603530884,\n",
              "  0.38370001316070557,\n",
              "  0.48979997634887695,\n",
              "  0.4020000100135803,\n",
              "  0.43309998512268066,\n",
              "  0.41269999742507935,\n",
              "  0.4146000146865845,\n",
              "  0.45660001039505005,\n",
              "  0.4366999864578247,\n",
              "  0.3889999985694885],\n",
              " 5: [0.5940999984741211,\n",
              "  0.6044000089168549,\n",
              "  0.5234000086784363,\n",
              "  0.49479997158050537,\n",
              "  0.49070000648498535,\n",
              "  0.49479997158050537,\n",
              "  0.480400025844574,\n",
              "  0.46139997243881226,\n",
              "  0.42079997062683105,\n",
              "  0.40630000829696655,\n",
              "  0.4077000021934509,\n",
              "  0.414900004863739,\n",
              "  0.4398000240325928,\n",
              "  0.4212999939918518,\n",
              "  0.4010000228881836,\n",
              "  0.40130001306533813,\n",
              "  0.3937000036239624,\n",
              "  0.392799973487854,\n",
              "  0.36159998178482056,\n",
              "  0.3726999759674072,\n",
              "  0.3716999888420105,\n",
              "  0.36799997091293335,\n",
              "  0.3504999876022339,\n",
              "  0.34359997510910034,\n",
              "  0.3831999897956848,\n",
              "  0.3403000235557556,\n",
              "  0.3327000141143799,\n",
              "  0.3361999988555908,\n",
              "  0.3381999731063843,\n",
              "  0.32569998502731323,\n",
              "  0.3504999876022339,\n",
              "  0.3328999876976013,\n",
              "  0.30959999561309814,\n",
              "  0.3539000153541565,\n",
              "  0.3240000009536743,\n",
              "  0.3059999942779541,\n",
              "  0.31220000982284546,\n",
              "  0.3148000240325928,\n",
              "  0.305400013923645,\n",
              "  0.3046000003814697,\n",
              "  0.3197000026702881,\n",
              "  0.2971000075340271,\n",
              "  0.2971000075340271,\n",
              "  0.3335999846458435,\n",
              "  0.3246999979019165,\n",
              "  0.29820001125335693,\n",
              "  0.3046000003814697,\n",
              "  0.328000009059906,\n",
              "  0.28939998149871826,\n",
              "  0.2939000129699707,\n",
              "  0.29509997367858887,\n",
              "  0.3296999931335449,\n",
              "  0.28299999237060547,\n",
              "  0.29110002517700195,\n",
              "  0.29670000076293945,\n",
              "  0.29030001163482666,\n",
              "  0.28060001134872437,\n",
              "  0.2940000295639038,\n",
              "  0.32429999113082886,\n",
              "  0.267300009727478,\n",
              "  0.3021000027656555,\n",
              "  0.2809000015258789,\n",
              "  0.28380000591278076,\n",
              "  0.31470000743865967,\n",
              "  0.27539998292922974,\n",
              "  0.27469998598098755,\n",
              "  0.3123999834060669,\n",
              "  0.28759998083114624,\n",
              "  0.267799973487854,\n",
              "  0.2799999713897705,\n",
              "  0.2821999788284302,\n",
              "  0.28390002250671387,\n",
              "  0.2634999752044678,\n",
              "  0.2889999747276306,\n",
              "  0.2742000222206116,\n",
              "  0.2678999900817871,\n",
              "  0.2842000126838684,\n",
              "  0.26419997215270996,\n",
              "  0.27060002088546753,\n",
              "  0.26639997959136963,\n",
              "  0.2897999882698059,\n",
              "  0.2775999903678894,\n",
              "  0.27399998903274536,\n",
              "  0.27240002155303955,\n",
              "  0.26840001344680786,\n",
              "  0.26319998502731323,\n",
              "  0.27399998903274536,\n",
              "  0.2996000051498413,\n",
              "  0.2710999846458435,\n",
              "  0.27549999952316284,\n",
              "  0.27490001916885376,\n",
              "  0.27389997243881226,\n",
              "  0.26670002937316895,\n",
              "  0.2605000138282776,\n",
              "  0.2749999761581421,\n",
              "  0.25700002908706665,\n",
              "  0.27410000562667847,\n",
              "  0.2501000165939331,\n",
              "  0.25770002603530884,\n",
              "  0.25529998540878296],\n",
              " 8: [0.6214999854564667,\n",
              "  0.597000002861023,\n",
              "  0.5045999884605408,\n",
              "  0.45499998331069946,\n",
              "  0.4236999750137329,\n",
              "  0.4096999764442444,\n",
              "  0.4067000150680542,\n",
              "  0.4067000150680542,\n",
              "  0.390999972820282,\n",
              "  0.35409998893737793,\n",
              "  0.3962000012397766,\n",
              "  0.34939998388290405,\n",
              "  0.3302000164985657,\n",
              "  0.3238999843597412,\n",
              "  0.3116000294685364,\n",
              "  0.32130002975463867,\n",
              "  0.29809999465942383,\n",
              "  0.31220000982284546,\n",
              "  0.2930999994277954,\n",
              "  0.31290000677108765,\n",
              "  0.29339998960494995,\n",
              "  0.2897999882698059,\n",
              "  0.2681000232696533,\n",
              "  0.2847999930381775,\n",
              "  0.27549999952316284,\n",
              "  0.27960002422332764,\n",
              "  0.29280000925064087,\n",
              "  0.2831000089645386,\n",
              "  0.27730000019073486,\n",
              "  0.2551000118255615,\n",
              "  0.2653999924659729,\n",
              "  0.2549999952316284,\n",
              "  0.26190000772476196,\n",
              "  0.27310001850128174,\n",
              "  0.2750999927520752,\n",
              "  0.25370001792907715,\n",
              "  0.2402999997138977,\n",
              "  0.2612000107765198,\n",
              "  0.2475000023841858,\n",
              "  0.24489998817443848,\n",
              "  0.25110000371932983,\n",
              "  0.2590000033378601,\n",
              "  0.2530999779701233,\n",
              "  0.2831000089645386,\n",
              "  0.24210000038146973,\n",
              "  0.2533000111579895,\n",
              "  0.24800002574920654,\n",
              "  0.2882999777793884,\n",
              "  0.25880002975463867,\n",
              "  0.24010002613067627,\n",
              "  0.26440000534057617,\n",
              "  0.23479998111724854,\n",
              "  0.256600022315979,\n",
              "  0.2402999997138977,\n",
              "  0.24819999933242798,\n",
              "  0.24559998512268066,\n",
              "  0.3165000081062317,\n",
              "  0.2436000108718872,\n",
              "  0.2337999939918518,\n",
              "  0.2581999897956848,\n",
              "  0.2524999976158142,\n",
              "  0.24210000038146973,\n",
              "  0.24570000171661377,\n",
              "  0.25120002031326294,\n",
              "  0.2329999804496765,\n",
              "  0.24709999561309814,\n",
              "  0.26319998502731323,\n",
              "  0.22070002555847168,\n",
              "  0.25120002031326294,\n",
              "  0.2458999752998352,\n",
              "  0.23500001430511475,\n",
              "  0.2465999722480774,\n",
              "  0.24239999055862427,\n",
              "  0.232200026512146,\n",
              "  0.24529999494552612,\n",
              "  0.24449998140335083,\n",
              "  0.2558000087738037,\n",
              "  0.23170000314712524,\n",
              "  0.2311999797821045,\n",
              "  0.22439998388290405,\n",
              "  0.22909998893737793,\n",
              "  0.21050000190734863,\n",
              "  0.20550000667572021,\n",
              "  0.20959997177124023,\n",
              "  0.20709997415542603,\n",
              "  0.20389997959136963,\n",
              "  0.20420002937316895,\n",
              "  0.20550000667572021,\n",
              "  0.2020999789237976,\n",
              "  0.20550000667572021,\n",
              "  0.20440000295639038,\n",
              "  0.20499998331069946,\n",
              "  0.2027999758720398,\n",
              "  0.20319998264312744,\n",
              "  0.2069000005722046,\n",
              "  0.20410001277923584,\n",
              "  0.20249998569488525,\n",
              "  0.2021999955177307,\n",
              "  0.20560002326965332,\n",
              "  0.20550000667572021],\n",
              " 10: [0.6500000059604645,\n",
              "  0.5302000045776367,\n",
              "  0.45730000734329224,\n",
              "  0.43449997901916504,\n",
              "  0.4154999852180481,\n",
              "  0.4090999960899353,\n",
              "  0.3687000274658203,\n",
              "  0.3992999792098999,\n",
              "  0.3151000142097473,\n",
              "  0.3536999821662903,\n",
              "  0.30970001220703125,\n",
              "  0.3077999949455261,\n",
              "  0.32669997215270996,\n",
              "  0.3292999863624573,\n",
              "  0.31370002031326294,\n",
              "  0.3044000267982483,\n",
              "  0.281000018119812,\n",
              "  0.3238999843597412,\n",
              "  0.27300000190734863,\n",
              "  0.2775999903678894,\n",
              "  0.2671999931335449,\n",
              "  0.2660999894142151,\n",
              "  0.29739999771118164,\n",
              "  0.2680000066757202,\n",
              "  0.32760000228881836,\n",
              "  0.2846999764442444,\n",
              "  0.25520002841949463,\n",
              "  0.2766000032424927,\n",
              "  0.24970000982284546,\n",
              "  0.265500009059906,\n",
              "  0.2695000171661377,\n",
              "  0.22530001401901245,\n",
              "  0.24470001459121704,\n",
              "  0.2534000277519226,\n",
              "  0.2548999786376953,\n",
              "  0.2508999705314636,\n",
              "  0.2800999879837036,\n",
              "  0.23739999532699585,\n",
              "  0.23519998788833618,\n",
              "  0.24720001220703125,\n",
              "  0.26270002126693726,\n",
              "  0.2868000268936157,\n",
              "  0.27960002422332764,\n",
              "  0.26840001344680786,\n",
              "  0.24250000715255737,\n",
              "  0.2343999743461609,\n",
              "  0.24180001020431519,\n",
              "  0.2598000168800354,\n",
              "  0.2498999834060669,\n",
              "  0.23110002279281616,\n",
              "  0.22589999437332153,\n",
              "  0.24889999628067017,\n",
              "  0.24010002613067627,\n",
              "  0.23079997301101685,\n",
              "  0.22200000286102295,\n",
              "  0.2361999750137329,\n",
              "  0.24940001964569092,\n",
              "  0.2184000015258789,\n",
              "  0.2368999719619751,\n",
              "  0.23580002784729004,\n",
              "  0.21549999713897705,\n",
              "  0.21640002727508545,\n",
              "  0.23430001735687256,\n",
              "  0.2321000099182129,\n",
              "  0.21740001440048218,\n",
              "  0.2239999771118164,\n",
              "  0.2339000105857849,\n",
              "  0.2369999885559082,\n",
              "  0.22369998693466187,\n",
              "  0.22670000791549683,\n",
              "  0.23030000925064087,\n",
              "  0.21420001983642578,\n",
              "  0.24129998683929443,\n",
              "  0.23019999265670776,\n",
              "  0.2120000123977661,\n",
              "  0.2347000241279602,\n",
              "  0.22460001707077026,\n",
              "  0.22420001029968262,\n",
              "  0.2314000129699707,\n",
              "  0.22390002012252808,\n",
              "  0.23650002479553223,\n",
              "  0.22200000286102295,\n",
              "  0.210099995136261,\n",
              "  0.23360002040863037,\n",
              "  0.22899997234344482,\n",
              "  0.21160000562667847,\n",
              "  0.21609997749328613,\n",
              "  0.22109997272491455,\n",
              "  0.22329998016357422,\n",
              "  0.21130001544952393,\n",
              "  0.21789997816085815,\n",
              "  0.22119998931884766,\n",
              "  0.22829997539520264,\n",
              "  0.225600004196167,\n",
              "  0.22439998388290405,\n",
              "  0.21619999408721924,\n",
              "  0.22289997339248657,\n",
              "  0.21109998226165771,\n",
              "  0.2062000036239624,\n",
              "  0.21310001611709595],\n",
              " 11: [0.5940000116825104,\n",
              "  0.4966999888420105,\n",
              "  0.414900004863739,\n",
              "  0.4869999885559082,\n",
              "  0.4147999882698059,\n",
              "  0.3687000274658203,\n",
              "  0.39670002460479736,\n",
              "  0.3148999810218811,\n",
              "  0.3418999910354614,\n",
              "  0.2975999712944031,\n",
              "  0.32020002603530884,\n",
              "  0.33230000734329224,\n",
              "  0.30809998512268066,\n",
              "  0.2833999991416931,\n",
              "  0.26889997720718384,\n",
              "  0.32120001316070557,\n",
              "  0.2939000129699707,\n",
              "  0.32340002059936523,\n",
              "  0.29329997301101685,\n",
              "  0.2572000026702881,\n",
              "  0.29830002784729004,\n",
              "  0.26020002365112305,\n",
              "  0.25919997692108154,\n",
              "  0.2555999755859375,\n",
              "  0.26590001583099365,\n",
              "  0.2900000214576721,\n",
              "  0.24650001525878906,\n",
              "  0.27230000495910645,\n",
              "  0.24470001459121704,\n",
              "  0.2645000219345093,\n",
              "  0.27149999141693115,\n",
              "  0.23900002241134644,\n",
              "  0.24000000953674316,\n",
              "  0.2656000256538391,\n",
              "  0.26340001821517944,\n",
              "  0.26179999113082886,\n",
              "  0.23110002279281616,\n",
              "  0.22909998893737793,\n",
              "  0.24290001392364502,\n",
              "  0.24839997291564941,\n",
              "  0.22790002822875977,\n",
              "  0.257099986076355,\n",
              "  0.218500018119812,\n",
              "  0.2368999719619751,\n",
              "  0.2175999879837036,\n",
              "  0.23040002584457397,\n",
              "  0.23229998350143433,\n",
              "  0.22280001640319824,\n",
              "  0.24010002613067627,\n",
              "  0.2102000117301941,\n",
              "  0.23629999160766602,\n",
              "  0.21450001001358032,\n",
              "  0.22089999914169312,\n",
              "  0.24580001831054688,\n",
              "  0.22610002756118774,\n",
              "  0.22009998559951782,\n",
              "  0.20099997520446777,\n",
              "  0.22659999132156372,\n",
              "  0.21359997987747192,\n",
              "  0.2214999794960022,\n",
              "  0.21219998598098755,\n",
              "  0.22269999980926514,\n",
              "  0.21740001440048218,\n",
              "  0.23229998350143433,\n",
              "  0.2127000093460083,\n",
              "  0.2182999849319458,\n",
              "  0.21880000829696655,\n",
              "  0.2085999846458435,\n",
              "  0.21609997749328613,\n",
              "  0.2037000060081482,\n",
              "  0.23650002479553223,\n",
              "  0.20990002155303955,\n",
              "  0.2028999924659729,\n",
              "  0.2346000075340271,\n",
              "  0.21069997549057007,\n",
              "  0.21039998531341553,\n",
              "  0.2192000150680542,\n",
              "  0.20789998769760132,\n",
              "  0.2149999737739563,\n",
              "  0.20759999752044678,\n",
              "  0.23070001602172852,\n",
              "  0.2156999707221985,\n",
              "  0.2254999876022339,\n",
              "  0.22939997911453247,\n",
              "  0.22790002822875977,\n",
              "  0.22579997777938843,\n",
              "  0.22909998893737793,\n",
              "  0.21880000829696655,\n",
              "  0.2222999930381775,\n",
              "  0.2249000072479248,\n",
              "  0.2190999984741211,\n",
              "  0.22049999237060547,\n",
              "  0.22100001573562622,\n",
              "  0.22710001468658447,\n",
              "  0.23479998111724854,\n",
              "  0.2069000005722046,\n",
              "  0.20910000801086426,\n",
              "  0.21230000257492065,\n",
              "  0.22210001945495605,\n",
              "  0.21679997444152832],\n",
              " 12: [0.5995999872684479,\n",
              "  0.5173999965190887,\n",
              "  0.4718000292778015,\n",
              "  0.4032999873161316,\n",
              "  0.3614000082015991,\n",
              "  0.3741999864578247,\n",
              "  0.33399999141693115,\n",
              "  0.3098999857902527,\n",
              "  0.3101999759674072,\n",
              "  0.33810001611709595,\n",
              "  0.31220000982284546,\n",
              "  0.3313000202178955,\n",
              "  0.2824000120162964,\n",
              "  0.30580002069473267,\n",
              "  0.3223000168800354,\n",
              "  0.26759999990463257,\n",
              "  0.29729998111724854,\n",
              "  0.25950002670288086,\n",
              "  0.25520002841949463,\n",
              "  0.24830001592636108,\n",
              "  0.2669000029563904,\n",
              "  0.267799973487854,\n",
              "  0.2540000081062317,\n",
              "  0.2538999915122986,\n",
              "  0.26670002937316895,\n",
              "  0.2580000162124634,\n",
              "  0.2648000121116638,\n",
              "  0.2562999725341797,\n",
              "  0.2315000295639038,\n",
              "  0.26260000467300415,\n",
              "  0.2628999948501587,\n",
              "  0.24049997329711914,\n",
              "  0.23159998655319214,\n",
              "  0.25849997997283936,\n",
              "  0.24119997024536133,\n",
              "  0.2379000186920166,\n",
              "  0.24400001764297485,\n",
              "  0.2468000054359436,\n",
              "  0.24720001220703125,\n",
              "  0.23030000925064087,\n",
              "  0.234499990940094,\n",
              "  0.22299998998641968,\n",
              "  0.2152000069618225,\n",
              "  0.22780001163482666,\n",
              "  0.23570001125335693,\n",
              "  0.21969997882843018,\n",
              "  0.23159998655319214,\n",
              "  0.24489998817443848,\n",
              "  0.2175999879837036,\n",
              "  0.21439999341964722,\n",
              "  0.2321000099182129,\n",
              "  0.2117999792098999,\n",
              "  0.22359997034072876,\n",
              "  0.22189998626708984,\n",
              "  0.232200026512146,\n",
              "  0.21679997444152832,\n",
              "  0.21319997310638428,\n",
              "  0.2491999864578247,\n",
              "  0.2401999831199646,\n",
              "  0.2175999879837036,\n",
              "  0.2246999740600586,\n",
              "  0.23989999294281006,\n",
              "  0.21950000524520874,\n",
              "  0.22829997539520264,\n",
              "  0.23189997673034668,\n",
              "  0.2530999779701233,\n",
              "  0.2087000012397766,\n",
              "  0.20420002937316895,\n",
              "  0.2142999768257141,\n",
              "  0.21160000562667847,\n",
              "  0.22420001029968262,\n",
              "  0.20719999074935913,\n",
              "  0.20429998636245728,\n",
              "  0.24769997596740723,\n",
              "  0.22839999198913574,\n",
              "  0.22899997234344482,\n",
              "  0.19919997453689575,\n",
              "  0.21390002965927124,\n",
              "  0.2087000012397766,\n",
              "  0.25449997186660767,\n",
              "  0.23659998178482056,\n",
              "  0.203000009059906,\n",
              "  0.19840002059936523,\n",
              "  0.22119998931884766,\n",
              "  0.2092999815940857,\n",
              "  0.19919997453689575,\n",
              "  0.22439998388290405,\n",
              "  0.2020999789237976,\n",
              "  0.22850000858306885,\n",
              "  0.21310001611709595,\n",
              "  0.20139998197555542,\n",
              "  0.2156999707221985,\n",
              "  0.22289997339248657,\n",
              "  0.2092999815940857,\n",
              "  0.20980000495910645,\n",
              "  0.20480000972747803,\n",
              "  0.19940000772476196,\n",
              "  0.20759999752044678,\n",
              "  0.23360002040863037,\n",
              "  0.24180001020431519],\n",
              " 13: [0.6019000113010406,\n",
              "  0.5047999918460846,\n",
              "  0.5175000131130219,\n",
              "  0.44849997758865356,\n",
              "  0.37800002098083496,\n",
              "  0.3646000027656555,\n",
              "  0.35759997367858887,\n",
              "  0.3589000105857849,\n",
              "  0.35860002040863037,\n",
              "  0.32889997959136963,\n",
              "  0.35750001668930054,\n",
              "  0.3895999789237976,\n",
              "  0.2954000234603882,\n",
              "  0.28450000286102295,\n",
              "  0.30239999294281006,\n",
              "  0.2985000014305115,\n",
              "  0.2735000252723694,\n",
              "  0.2930999994277954,\n",
              "  0.2621999979019165,\n",
              "  0.2986000180244446,\n",
              "  0.2670000195503235,\n",
              "  0.25620001554489136,\n",
              "  0.23970001935958862,\n",
              "  0.2735000252723694,\n",
              "  0.2613000273704529,\n",
              "  0.25870001316070557,\n",
              "  0.2555999755859375,\n",
              "  0.23739999532699585,\n",
              "  0.2493000030517578,\n",
              "  0.2718999981880188,\n",
              "  0.23309999704360962,\n",
              "  0.23610001802444458,\n",
              "  0.22869998216629028,\n",
              "  0.21549999713897705,\n",
              "  0.23909997940063477,\n",
              "  0.22769999504089355,\n",
              "  0.23760002851486206,\n",
              "  0.21640002727508545,\n",
              "  0.22680002450942993,\n",
              "  0.22369998693466187,\n",
              "  0.22619998455047607,\n",
              "  0.2103000283241272,\n",
              "  0.23309999704360962,\n",
              "  0.25269997119903564,\n",
              "  0.22899997234344482,\n",
              "  0.23750001192092896,\n",
              "  0.2257000207901001,\n",
              "  0.22200000286102295,\n",
              "  0.21420001983642578,\n",
              "  0.22109997272491455,\n",
              "  0.21929997205734253,\n",
              "  0.22310000658035278,\n",
              "  0.23500001430511475,\n",
              "  0.21240001916885376,\n",
              "  0.2059999704360962,\n",
              "  0.23240000009536743,\n",
              "  0.22439998388290405,\n",
              "  0.23009997606277466,\n",
              "  0.21399998664855957,\n",
              "  0.21960002183914185,\n",
              "  0.21119999885559082,\n",
              "  0.22219997644424438,\n",
              "  0.21460002660751343,\n",
              "  0.21329998970031738,\n",
              "  0.21109998226165771,\n",
              "  0.21039998531341553,\n",
              "  0.20550000667572021,\n",
              "  0.21560001373291016,\n",
              "  0.21740001440048218,\n",
              "  0.21289998292922974,\n",
              "  0.20660001039505005,\n",
              "  0.20959997177124023,\n",
              "  0.2103000283241272,\n",
              "  0.2095000147819519,\n",
              "  0.22130000591278076,\n",
              "  0.2062000036239624,\n",
              "  0.21119999885559082,\n",
              "  0.20550000667572021,\n",
              "  0.21439999341964722,\n",
              "  0.2142999768257141,\n",
              "  0.20020002126693726,\n",
              "  0.1965000033378601,\n",
              "  0.21299999952316284,\n",
              "  0.22670000791549683,\n",
              "  0.23919999599456787,\n",
              "  0.19550001621246338,\n",
              "  0.21079999208450317,\n",
              "  0.20810002088546753,\n",
              "  0.20819997787475586,\n",
              "  0.20569998025894165,\n",
              "  0.21530002355575562,\n",
              "  0.21490001678466797,\n",
              "  0.21689999103546143,\n",
              "  0.1955999732017517,\n",
              "  0.20829999446868896,\n",
              "  0.20670002698898315,\n",
              "  0.21399998664855957,\n",
              "  0.21609997749328613,\n",
              "  0.21670001745224,\n",
              "  0.20069998502731323],\n",
              " 16: [0.5142000019550323,\n",
              "  0.5320999920368195,\n",
              "  0.4433000087738037,\n",
              "  0.4085000157356262,\n",
              "  0.36900001764297485,\n",
              "  0.39730000495910645,\n",
              "  0.3773000240325928,\n",
              "  0.3148999810218811,\n",
              "  0.2937999963760376,\n",
              "  0.3125,\n",
              "  0.265500009059906,\n",
              "  0.2757999897003174,\n",
              "  0.2864000201225281,\n",
              "  0.2750999927520752,\n",
              "  0.25449997186660767,\n",
              "  0.2718999981880188,\n",
              "  0.25599998235702515,\n",
              "  0.2703999876976013,\n",
              "  0.2843000292778015,\n",
              "  0.22939997911453247,\n",
              "  0.24940001964569092,\n",
              "  0.24049997329711914,\n",
              "  0.25950002670288086,\n",
              "  0.2523999810218811,\n",
              "  0.23420000076293945,\n",
              "  0.26660001277923584,\n",
              "  0.22189998626708984,\n",
              "  0.22060000896453857,\n",
              "  0.23580002784729004,\n",
              "  0.22659999132156372,\n",
              "  0.2312999963760376,\n",
              "  0.2548999786376953,\n",
              "  0.22769999504089355,\n",
              "  0.2160000205039978,\n",
              "  0.2329999804496765,\n",
              "  0.22619998455047607,\n",
              "  0.20910000801086426,\n",
              "  0.20899999141693115,\n",
              "  0.2906000018119812,\n",
              "  0.21060001850128174,\n",
              "  0.20670002698898315,\n",
              "  0.21399998664855957,\n",
              "  0.21560001373291016,\n",
              "  0.27090001106262207,\n",
              "  0.2214999794960022,\n",
              "  0.22649997472763062,\n",
              "  0.20560002326965332,\n",
              "  0.19809997081756592,\n",
              "  0.20789998769760132,\n",
              "  0.2125999927520752,\n",
              "  0.21859997510910034,\n",
              "  0.23760002851486206,\n",
              "  0.2060999870300293,\n",
              "  0.21799999475479126,\n",
              "  0.21530002355575562,\n",
              "  0.20999997854232788,\n",
              "  0.21439999341964722,\n",
              "  0.20160001516342163,\n",
              "  0.21130001544952393,\n",
              "  0.24059998989105225,\n",
              "  0.22380000352859497,\n",
              "  0.2192000150680542,\n",
              "  0.20249998569488525,\n",
              "  0.20490002632141113,\n",
              "  0.20340001583099365,\n",
              "  0.1923999786376953,\n",
              "  0.21890002489089966,\n",
              "  0.2200000286102295,\n",
              "  0.20480000972747803,\n",
              "  0.2085999846458435,\n",
              "  0.20270001888275146,\n",
              "  0.21130001544952393,\n",
              "  0.1923999786376953,\n",
              "  0.225600004196167,\n",
              "  0.20730000734329224,\n",
              "  0.19590002298355103,\n",
              "  0.22329998016357422,\n",
              "  0.20480000972747803,\n",
              "  0.20670002698898315,\n",
              "  0.2174999713897705,\n",
              "  0.20889997482299805,\n",
              "  0.17079997062683105,\n",
              "  0.1689000129699707,\n",
              "  0.16990000009536743,\n",
              "  0.17079997062683105,\n",
              "  0.16839998960494995,\n",
              "  0.17100000381469727,\n",
              "  0.16759997606277466,\n",
              "  0.16769999265670776,\n",
              "  0.16850000619888306,\n",
              "  0.16729998588562012,\n",
              "  0.16909998655319214,\n",
              "  0.1679999828338623,\n",
              "  0.16759997606277466,\n",
              "  0.16670000553131104,\n",
              "  0.16589999198913574,\n",
              "  0.16649997234344482,\n",
              "  0.16600000858306885,\n",
              "  0.17180001735687256,\n",
              "  0.1689000129699707],\n",
              " 24: [0.5397000014781952,\n",
              "  0.4323999881744385,\n",
              "  0.477400004863739,\n",
              "  0.39889997243881226,\n",
              "  0.42669999599456787,\n",
              "  0.3228999972343445,\n",
              "  0.30949997901916504,\n",
              "  0.3312000036239624,\n",
              "  0.32359999418258667,\n",
              "  0.32819998264312744,\n",
              "  0.2728999853134155,\n",
              "  0.28130000829696655,\n",
              "  0.32499998807907104,\n",
              "  0.28329998254776,\n",
              "  0.2847999930381775,\n",
              "  0.25029999017715454,\n",
              "  0.24879997968673706,\n",
              "  0.24199998378753662,\n",
              "  0.234499990940094,\n",
              "  0.2271999716758728,\n",
              "  0.2190999984741211,\n",
              "  0.21410000324249268,\n",
              "  0.2994999885559082,\n",
              "  0.22519999742507935,\n",
              "  0.2249000072479248,\n",
              "  0.24299997091293335,\n",
              "  0.22269999980926514,\n",
              "  0.20520001649856567,\n",
              "  0.21799999475479126,\n",
              "  0.203000009059906,\n",
              "  0.21390002965927124,\n",
              "  0.17930001020431519,\n",
              "  0.20749998092651367,\n",
              "  0.2311999797821045,\n",
              "  0.2037000060081482,\n",
              "  0.22420001029968262,\n",
              "  0.21050000190734863,\n",
              "  0.20099997520446777,\n",
              "  0.2092999815940857,\n",
              "  0.21700000762939453,\n",
              "  0.19590002298355103,\n",
              "  0.1955999732017517,\n",
              "  0.2077999711036682,\n",
              "  0.21890002489089966,\n",
              "  0.21670001745224,\n",
              "  0.20920002460479736,\n",
              "  0.20759999752044678,\n",
              "  0.19539999961853027,\n",
              "  0.18220001459121704,\n",
              "  0.1923999786376953,\n",
              "  0.18639999628067017,\n",
              "  0.2013000249862671,\n",
              "  0.1873999834060669,\n",
              "  0.19809997081756592,\n",
              "  0.20829999446868896,\n",
              "  0.21789997816085815,\n",
              "  0.18529999256134033,\n",
              "  0.19419997930526733,\n",
              "  0.20880001783370972,\n",
              "  0.18949997425079346,\n",
              "  0.18699997663497925,\n",
              "  0.19419997930526733,\n",
              "  0.18239998817443848,\n",
              "  0.19139999151229858,\n",
              "  0.18930000066757202,\n",
              "  0.2475000023841858,\n",
              "  0.18540000915527344,\n",
              "  0.21700000762939453,\n",
              "  0.19929999113082886,\n",
              "  0.19450002908706665,\n",
              "  0.17949998378753662,\n",
              "  0.18550002574920654,\n",
              "  0.18980002403259277,\n",
              "  0.1834999918937683,\n",
              "  0.17879998683929443,\n",
              "  0.19789999723434448,\n",
              "  0.18559998273849487,\n",
              "  0.19819998741149902,\n",
              "  0.1998000144958496,\n",
              "  0.18480002880096436,\n",
              "  0.18529999256134033,\n",
              "  0.19230002164840698,\n",
              "  0.18639999628067017,\n",
              "  0.1980000138282776,\n",
              "  0.19249999523162842,\n",
              "  0.19220000505447388,\n",
              "  0.18580001592636108,\n",
              "  0.18720000982284546,\n",
              "  0.18809998035430908,\n",
              "  0.2003999948501587,\n",
              "  0.18940001726150513,\n",
              "  0.18870002031326294,\n",
              "  0.18290001153945923,\n",
              "  0.18229997158050537,\n",
              "  0.18709999322891235,\n",
              "  0.18639999628067017,\n",
              "  0.21859997510910034,\n",
              "  0.19830000400543213,\n",
              "  0.18370002508163452,\n",
              "  0.1841999888420105],\n",
              " 32: [0.5144999921321869,\n",
              "  0.4496999979019165,\n",
              "  0.4065999984741211,\n",
              "  0.37220001220703125,\n",
              "  0.3367999792098999,\n",
              "  0.31709998846054077,\n",
              "  0.3992000222206116,\n",
              "  0.2815999984741211,\n",
              "  0.2976999878883362,\n",
              "  0.3183000087738037,\n",
              "  0.2815999984741211,\n",
              "  0.2985000014305115,\n",
              "  0.3460000157356262,\n",
              "  0.28949999809265137,\n",
              "  0.29409998655319214,\n",
              "  0.2190999984741211,\n",
              "  0.21039998531341553,\n",
              "  0.25050002336502075,\n",
              "  0.2339000105857849,\n",
              "  0.2354000210762024,\n",
              "  0.23489999771118164,\n",
              "  0.23519998788833618,\n",
              "  0.21170002222061157,\n",
              "  0.21890002489089966,\n",
              "  0.19620001316070557,\n",
              "  0.19830000400543213,\n",
              "  0.20999997854232788,\n",
              "  0.20349997282028198,\n",
              "  0.23170000314712524,\n",
              "  0.21420001983642578,\n",
              "  0.2117999792098999,\n",
              "  0.2037000060081482,\n",
              "  0.18040001392364502,\n",
              "  0.20010000467300415,\n",
              "  0.2369999885559082,\n",
              "  0.18470001220703125,\n",
              "  0.20740002393722534,\n",
              "  0.2436000108718872,\n",
              "  0.2200000286102295,\n",
              "  0.21139997243881226,\n",
              "  0.2095000147819519,\n",
              "  0.19880002737045288,\n",
              "  0.19789999723434448,\n",
              "  0.19459998607635498,\n",
              "  0.21369999647140503,\n",
              "  0.194100022315979,\n",
              "  0.2134000062942505,\n",
              "  0.19470000267028809,\n",
              "  0.20389997959136963,\n",
              "  0.1883000135421753,\n",
              "  0.19169998168945312,\n",
              "  0.18029999732971191,\n",
              "  0.19349998235702515,\n",
              "  0.20719999074935913,\n",
              "  0.1809999942779541,\n",
              "  0.20429998636245728,\n",
              "  0.20670002698898315,\n",
              "  0.1840999722480774,\n",
              "  0.19260001182556152,\n",
              "  0.18660002946853638,\n",
              "  0.19129997491836548,\n",
              "  0.18610000610351562,\n",
              "  0.19900000095367432,\n",
              "  0.19359999895095825,\n",
              "  0.20319998264312744,\n",
              "  0.19529998302459717,\n",
              "  0.16860002279281616,\n",
              "  0.17970001697540283,\n",
              "  0.18269997835159302,\n",
              "  0.18660002946853638,\n",
              "  0.19609999656677246,\n",
              "  0.1753000020980835,\n",
              "  0.1940000057220459,\n",
              "  0.169700026512146,\n",
              "  0.16860002279281616,\n",
              "  0.18389999866485596,\n",
              "  0.16509997844696045,\n",
              "  0.2021999955177307,\n",
              "  0.18730002641677856,\n",
              "  0.19910001754760742,\n",
              "  0.17919999361038208,\n",
              "  0.1875,\n",
              "  0.17820000648498535,\n",
              "  0.17250001430511475,\n",
              "  0.1801999807357788,\n",
              "  0.17259997129440308,\n",
              "  0.18449997901916504,\n",
              "  0.17619997262954712,\n",
              "  0.18790000677108765,\n",
              "  0.17350000143051147,\n",
              "  0.1712999939918518,\n",
              "  0.1657000184059143,\n",
              "  0.17629998922348022,\n",
              "  0.16610002517700195,\n",
              "  0.1965000033378601,\n",
              "  0.17989999055862427,\n",
              "  0.18790000677108765,\n",
              "  0.1761000156402588,\n",
              "  0.1680999994277954,\n",
              "  0.17419999837875366],\n",
              " 64: [0.5220000147819519,\n",
              "  0.5514000058174133,\n",
              "  0.39730000495910645,\n",
              "  0.4074000120162964,\n",
              "  0.33550000190734863,\n",
              "  0.3357999920845032,\n",
              "  0.26340001821517944,\n",
              "  0.26099997758865356,\n",
              "  0.29680001735687256,\n",
              "  0.28710001707077026,\n",
              "  0.25849997997283936,\n",
              "  0.2583000063896179,\n",
              "  0.2875000238418579,\n",
              "  0.24010002613067627,\n",
              "  0.21679997444152832,\n",
              "  0.24769997596740723,\n",
              "  0.23250001668930054,\n",
              "  0.21660000085830688,\n",
              "  0.23729997873306274,\n",
              "  0.21710002422332764,\n",
              "  0.19349998235702515,\n",
              "  0.2092999815940857,\n",
              "  0.21689999103546143,\n",
              "  0.2247999906539917,\n",
              "  0.2005000114440918,\n",
              "  0.20660001039505005,\n",
              "  0.21609997749328613,\n",
              "  0.24309998750686646,\n",
              "  0.18870002031326294,\n",
              "  0.20459997653961182,\n",
              "  0.18269997835159302,\n",
              "  0.1844000220298767,\n",
              "  0.19040000438690186,\n",
              "  0.18389999866485596,\n",
              "  0.17890000343322754,\n",
              "  0.18070000410079956,\n",
              "  0.1906999945640564,\n",
              "  0.19120001792907715,\n",
              "  0.18059998750686646,\n",
              "  0.1833999752998352,\n",
              "  0.20029997825622559,\n",
              "  0.19120001792907715,\n",
              "  0.1970999836921692,\n",
              "  0.18620002269744873,\n",
              "  0.20579999685287476,\n",
              "  0.18900001049041748,\n",
              "  0.17820000648498535,\n",
              "  0.18199998140335083,\n",
              "  0.2003999948501587,\n",
              "  0.19010001420974731,\n",
              "  0.178600013256073,\n",
              "  0.18449997901916504,\n",
              "  0.17320001125335693,\n",
              "  0.19809997081756592,\n",
              "  0.17729997634887695,\n",
              "  0.17170000076293945,\n",
              "  0.1899999976158142,\n",
              "  0.1779000163078308,\n",
              "  0.17400002479553223,\n",
              "  0.16990000009536743,\n",
              "  0.16170001029968262,\n",
              "  0.18620002269744873,\n",
              "  0.17239999771118164,\n",
              "  0.17040002346038818,\n",
              "  0.16289997100830078,\n",
              "  0.16339999437332153,\n",
              "  0.18140000104904175,\n",
              "  0.18839997053146362,\n",
              "  0.18070000410079956,\n",
              "  0.1729000210762024,\n",
              "  0.16610002517700195,\n",
              "  0.1751999855041504,\n",
              "  0.17570000886917114,\n",
              "  0.17239999771118164,\n",
              "  0.16649997234344482,\n",
              "  0.17229998111724854,\n",
              "  0.17650002241134644,\n",
              "  0.16390001773834229,\n",
              "  0.16509997844696045,\n",
              "  0.17180001735687256,\n",
              "  0.1567000150680542,\n",
              "  0.1728000044822693,\n",
              "  0.16509997844696045,\n",
              "  0.17799997329711914,\n",
              "  0.17799997329711914,\n",
              "  0.1654999852180481,\n",
              "  0.16380000114440918,\n",
              "  0.1589999794960022,\n",
              "  0.16509997844696045,\n",
              "  0.18250000476837158,\n",
              "  0.1769999861717224,\n",
              "  0.17909997701644897,\n",
              "  0.1679999828338623,\n",
              "  0.16680002212524414,\n",
              "  0.16680002212524414,\n",
              "  0.17269998788833618,\n",
              "  0.1696000099182129,\n",
              "  0.16269999742507935,\n",
              "  0.15570002794265747,\n",
              "  0.17000001668930054]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu_aWTcg9cL4",
        "colab_type": "code",
        "outputId": "fd12039d-f553-42ce-90a7-541d9aee6dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "train_error={1:0.5186,\n",
        "             2:0.6356,\n",
        "             8:0.8833,\n",
        "             11:0.9029,\n",
        "             12:09201,\n",
        "             13:0.9219,\n",
        "             32:0.9768,\n",
        "             64:0.9842}\n",
        "train_l =sorted(train_error.items())\n",
        "x1,y1 = zip(*train_l)\n",
        "plt.plot(x1,y1,label='Resnetv2')\n",
        "  \n",
        "plt.xlabel('Width Parameter')\n",
        "plt.ylabel('Train Error')\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-80-74cf9ec9efa7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    train_error={1:0.5186,2:0.6356,8:0.8833,11:0.9029,12:09201,13:0.9219,32:0.9768,64:0.9842}\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGvCW2zi_VNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_l =sorted(test_error.items())\n",
        "x,y = zip(*test_l)\n",
        "plt.plot(x,y,label='Resnetv2')\n",
        "  \n",
        "plt.xlabel('Width Parameter')\n",
        "plt.ylabel('Test Error')\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhPYy3DRWfjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_error[3]= 1-0.705299973487854"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPTx5RaM3NwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_error[8]=0.2055000066757202"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBmM9entdu7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}